Link: https://www.se-radio.net/2019/02/se-radio-episode-354-avi-kivity-on-scylladb/



This is Software Engineering Radio, the podcast for professional developers on the web at
se-radio.net.
SE Radio is brought to you by the IEEE Computer Society, the IEEE Software Magazine.
Online at computer.org slash software.
Hello, this is Nishant Soneja from Software Engineering Radio.
Today I am speaking with Avi Kaviti from SillaDB.
Hi Avi.
Hi Nishant, glad to be here.
Great.
Avi was a lead developer for Kumra Net Inc, the company that created the well-known KVM
project, the hypervisor underlying many production clouds.
Kumra Net Inc was acquired by Red Heart in 2008.
But after, Avi worked as a lead developer maintainer of the open-source KVM project until December
of 2012.
Avi is now the CTO of SillaDB, a company that seeks to bring the same kind of innovation
to the public cloud space.
So Avi, is there anything which I missed which you think is relevant in your past experience?
I also have had experience in Exynet, which was a company that was building distributed
storage, high performance distributed storage.
And that's of course relevant to building high performance distributed databases.
It's maybe one step down from distributed databases.
Definitely.
Thanks for filling in that missing gap.
So today we will be talking at length about the technology which is powering SillaDB.
To get started, Avi, can you describe briefly what is SillaDB?
So SillaDB is a distributed no-secure database.
It's compatible with Cassandra.
And the three main characteristics that I would say is first of all the compatibility
with Cassandra, which we try to make sure it's very compatible to make sure that it's easy
to use and to adopt the high performance, perhaps the most well-known feature, so that
you can get the most out of your hardware and you can bring low latency and high throughput
to your application.
And the third is Autonomous.
So SillaDB tries to tune itself as much as it can, adapt itself to the workload so that
you don't need to keep fiddling with configuration parameters.
Great.
That's a good summary of at high level what SillaDB does.
So one thing which I was always curious about is the motivation behind building the SillaDB.
Perhaps you can give us a high-level view of what you set out to do in building Silla.
Yeah, so the origin story of SillaDB is that the company was actually doing something else
some years ago.
We were building OSP, which is a cloud operating system.
And one of the workloads that we set out to optimize was Cassandra.
We knew it was a popular database and heavily used at many places.
So we ran Cassandra on top of OSV and tried to see what performance gains we could bring
to it.
And what we saw was that there was nothing at the operating system level that we could
do to improve Cassandra because it was busy.
Most of the time it was busy fighting with itself.
Either it was garbage collecting or the threads were busy taking locks or it was using the
file system in a way which could not be optimized.
So we saw a lot of bad performance characteristics in Cassandra and we did like the architecture.
They shared nothing and fully symmetric architecture.
So the combination of good architecture but lacking implementation led us to see that
there was an opportunity here.
And of course there was something else that led us to that.
It was that OSV was not doing so well.
So we decided to pivot.
And ever since then we've been doing SillaDB.
So you talked about a few issues which you encountered with Cassandra.
Were there any use cases in retrospections which you think that Cassandra is well suited
for perhaps over SillaDB?
So of course we tried to make sure that there are no such use cases.
We tried to make sure that we were always better than Cassandra.
Always making a better utilization of resources and delivering better latency and better manageability.
Still there are some cases where there are features at Cassandra which we did not yet
implement.
So in those cases it might be a good idea to start with Cassandra and perhaps move to
SillaDB when those features are implemented and when your use cases grown enough that
you need the power of SillaDB.
I see.
So basically I would want to use Cassandra if there is a new feature in Cassandra which
is not yet ported over to SillaDB.
Otherwise I should stick with Silla.
That's my bias recommendation.
So to set the stage and have the listeners get a motivation of SillaDB project can you
briefly share with us some statistics around throughput and latency comparison of SillaDB
with Cassandra?
So we've published several benchmarks that show that you can reach a million operations
per node and of course it scales horizontally so as you add more nodes your throughput
grows with number of nodes.
This is just a general number so of course it depends on the workload if you have complex
operations or operations that involve large amounts of or larger rows then you will see
lower throughput with lower number of operations per per node if the operations are simpler
than you can see higher throughput and of course throughput is not just about operations
per second it's also about how the database does its own internal maintenance things like
repair, things like bootstrapping and you know so all of these need to be taken into
consideration and we have benchmarks and blog post that show all of them.
Right I was going through the benchmarks on the SillaDB website and I came across the
Yahoo cloud serving benchmark and the numbers were pretty impressive.
The throughput which SillaDB was achieving on a three node cluster Cassandra was achieving
on a 30 node cluster with lower P99 latencies for an enterprise this basically means 10
times cost saving so which I personally found very remarkable.
Yes so of course the less hardware you need then the more savings you get or sometimes
conversely with the same amount of hardware you can do more or achieve lower throughput
we typically see a mix so people don't scale down the hardware as much as they can in order
to get more reserve and to get better latency on their throughput so it's a mix of better
latency, better throughput and reduced node count which drive migrations.
Great so let's dive in details in each of the fundamentals which make SillaDB faster
and more scalable than Cassandra.
I'm sure our listeners will be interested in knowing what makes SillaDB faster.
So one primary difference between the two databases is the language in which they are
implemented.
SillaDB in C++ and Cassandra in Java.
What was the motivation behind choosing C++ over Java?
So the number one motivation was control.
Java is a managed language it runs in the JVM and the JVM mediates everything that you
do with respect to the hardware and the operating system.
In contrast C++ allows you to do anything you like.
It also allows you to do bad things like accessing the free memory and shooting yourself in the
foot in general but it also allows you to do good things so you can lay out data in memory
in whichever way you like.
You can access any OS function or bypassing OS.
You have excellent control.
There's also the advantage in general it's faster but that was not the main motivation
and it's not a huge advantage.
So world written Java code can approach the performance of C++ code but it can never
approach the amount of control that C++ gives you.
Great.
So in the past we have done a few shows where we have discussed garbage collection and
it's impact on tail latencies of an application.
I have referenced them in the show notes.
I was interested in hearing from you as to what is the impact of GC pauses on a database
like SillaDB.
If it was implemented in Java in this case Cassandra will be more relevant.
Yes, so for Cassandra of course garbage collection is one of the main Achilles hills.
It's well known.
You can see it in the user list and you can hear complaints from users about garbage collection
that can pause a note for hundreds of milliseconds at a time and even more.
It's usually doing unpredictable things like suddenly you materialize a large partition
and it that effort creates a lot of garbage and the database has to pause and stop responding
to request in a distributed environment that can also cause problems that propagate across
the cluster because suddenly that node is not responding to other nodes and of course
if your client use case is latency sensitive then you feel the impact immediately.
You have no response from that node for those hundreds of milliseconds and you cannot continue
your workload.
So one of the tasks which GC also performs is heap compaction.
Without the presence of a garbage collector how does Silla manage the heap fragmentation?
That's an excellent question.
So many C or C++ databases or in-memory databases don't do compaction and as a result they can
reach a problem where you're changing the average size of the data that you store.
You can run into heap fragmentation and then when you try to store a larger object you
end up having to evict your entire cache in order to find enough contiguous space to
store the new data and of course we didn't want that to happen with the SillaDB.
So we use a combination of methods to avoid that.
The most important is that instead of using a regular allocator we have an additional allocator
which is called the log structured allocator similar to using a log structured merge tree
in storing the data.
So the way that the log structured allocator works is when we allocate we always allocate
to a new position and when we free we do not immediately reuse the free data and in that
case in that regard it's similar to garbage collection.
And the log structured allocator is aware of the location and the data types of all of
the data that's stored in it.
So it is able to move the data whenever it wants and this way it can compact the heap.
It uses 128k segments to store this data and that also means that we fragment larger data.
So if the user access to store a blob that is a megabyte in size we will spread that
blob over many different smaller blobs and of course keep track of them and this gives
the allocator flexibility to move those sub fragments around.
On demand it can free memory and give us those large 128k segments whenever there is need
for a new allocation.
Right so when the allocator moves around these fragments doesn't we have the same scenario
as Java garbage collection where we will be pausing the existing application threads.
So first of all the log structure allocator is local to a shard or a thread so it's completely
unrelated to the others.
And second it's similar to a garbage collector in that it compacts but it's not similar
in that we already know all of the free data.
We don't need to traverse the live set like a garbage collector that in order to find
out what is live and then understand that everything else is a garbage.
You already know what the garbage is.
It's explicitly free.
It's also only user objects that are stored in the log structure allocator.
Regular objects that are used as part of a request and responses and for general housekeeping
of the database those are allocated using the regular allocator so they don't participate
in this.
And this allows us to place bounds on the amount of time it takes us to evict things.
It's not under control over some general purpose garbage collector which is not aware of our
requirements.
It's under our control of the log structure allocators control so we know exactly when
is the right time to initiate compaction.
Great.
This was a good discussion on memory management and seller.
So moving on, was there any particular reason for choosing C++14 over more battle tested
C++11?
What are the differences between them which you think are discussion worthy?
So C++14 is an incremental update over C++11 but in fact we did have a reason to use C++14
and that is move capture in lambdas.
So we use lambdas extensively since we are an asynchronous application and lambdas are
generally very useful for asynchronous applications and the ability to use move captures means
that we can avoid copying data in continuations and that improves the performance quite a
lot.
This one feature that causes to use C++14 but by now we have moved to C++17 which has
a lot of very nice features but none of them were as compelling.
So we could have continued using C++14 but chose to update in order to enjoy the quality
of life benefits.
So you mentioned about Celas asynchronous model.
Let's discuss more about it.
So we hear a lot about Celas share nothing architecture.
Before we talk about Celas share nothing architecture and how is it designed.
Let's talk about share nothing architecture in general.
What according to you is a share nothing architecture when it comes to software engineering and
computer architecture?
So for that we have to look into the past where we have shared something architectures
and typically that something was shared storage.
So you would have a very expensive storage area network that you can access from multiple
computers and before that you had the double-tail SCSI that so you had a disk with multiple
interfaces that could connect to multiple SCSI buses and this way was used to implement
clusters.
So you had a shared pool of storage connected to multiple machines which could each access
the storage and that has the advantage of not requiring the users to think about partitioning
the data.
So no matter how the data was organized you could always fit it on your shared storage
pool but it has a disadvantage that storage was relatively exotic and also very expensive.
So people moved out of that in order to use commodity storage, much more inexpensive direct
to touch storage.
It's also much faster because everything is connected directly and doesn't have to go
through storage area networks.
So the combination of price and performance caused the industry to move away from shared
storage into shared nothing.
So has the shared nothing architecture become more important in recent times where the per
core clock cycles per seconds are plateauing and storage and networking devices are becoming
faster?
Is that one of the primary reasons which is driving this movement?
So first of all let's make a distinction.
SillaDB uses the shared nothing architecture in two levels.
The first level is shared nothing among nodes and that's the same as Cassandra.
You have a number of nodes.
Each of them is using its own storage and only communicating via the network instead
of using shared storage and this is more unique to SillaDB.
It also uses shared nothing architecture among the cores.
So each core only accesses its own memory, its own files and usually its own connection
connections.
So this brings the same advantages.
It reduces the need for locking which is expensive in shared architecture and it allows it to
use a large number of cores that are available.
It looks like the core war has restarted and now we have servers from AMD with 48 cores
per chip and I think now even 64 cores per chip.
And Intel is also starting launched cores CPUs with 56 cores per chip.
So you have very large numbers of cores and trying to write an application that can take
advantage of all of those cores with a traditional locking architecture is almost impossible.
It is very hard to get that right.
Sure.
So shared nothing on a node level is extremely important these days.
I see.
So I think you briefly talked about it.
But what do you think is a disadvantage of having multiple application threads per core?
So when you have many application threads then it means that the scheduler, the kernel scheduler
has to make the decision about which thread gets to run each time.
And of course that means that you lose control.
You're moving the decision to a component that is general purpose which of course well
designed and tested in many, many places but in something like a database you do need your
control.
You might hear me say that more than once on this talk.
Another problem is of course that in order to mediate access to data you will need locks
and locks are becoming more and more expensive and when you transition from one thread to
another again you go through the kernel and in these days of a meltdown and specter context
switches become more and more expensive.
The kernel has to make sure that the state is not leaked every time you transfer to the
kernel and back.
So those context switches are becoming slower over time instead of becoming faster.
So by having exactly one thread per core the kernel does not need to make any decision
about which thread to run just always one thread and all of those costs that are involved
with switching threads all of those are gone.
Great.
So in Silla if two cores do require to communicate we will discuss why that would be necessary
but if they do require to communicate how do they do it because locking is something
which we don't want.
So what is a communication mechanism for application threads where one application is running on
each core in SillaDB.
So every pair of cores is connected by a pair of single producers single consumer queues
and those are well understood queues that can be implemented without any locks just
with the memory barriers.
So when one core wants another core to do something on its behalf it places the message
into the queue in due time that other core will pull the message out of the queue, perform
the operation that was requested and place a response in the return queue.
And that may sound like a lot of work but when you apply batching to that then each
other operation becomes amortized because when you pull messages from a queue you're
actually pulling a lot of messages not just one and that reduces the cost per message
by quite a lot and you have really high throughput doing that.
So one question which I was wondering is since Silla is doing everything on a per core basis
the database sharding is done on a per core basis and as we will discuss further the network
stack is also on a per core basis why would intercore communication be necessary in the
first place?
It's a great question and in fact we are trying to reduce the amount of intercore communication.
The most common place where we have that is the client connections.
So when the client connects to a node it doesn't know that which sharded is connected to and
so it will send requests that are targeted at that node but not at a particular shard
and this way we might have a request arriving at one shard but which actually needs to be
processed by a different shard or on another core.
And that was the main cause for our intercore communications and in fact we added an enhancement
to the product call and to the drivers which allow each client driver to connect to generate
one connection per core and this way it can send the request directly at the core to service
it.
So in fact on new RCL implementations you will see much less intercore communications but
of course sometimes you are using an older driver and client update so you might still
see it.
We are working hard to eliminate all of those intercore communications not only reduce
performance but they also generate imbalance and imbalance is a pain of thread per core
implementations.
We like to see the workload evenly balanced among all the cores and all the nodes.
Great.
So let's talk about the C-Star framework.
The C-Star framework powers a lot of technology behind a CillaDB.
So can you explain what is C-Star framework and what is its relationship with Cilla?
So C-Star is a generic framework for thread per core applications.
It's oriented at server applications that do a mix of multi core of IO and networking.
So applications that do networking you have many applications these days that are oriented
that networking that do asynchronous networking but very few frameworks that do asynchronous
IO and that fully utilize a thread per core architecture.
So things like distributed databases, distributed file systems or other applications that are
storage intensive will benefit from C-Star.
Obviously it's co-developed with the Cilla since whenever we need a new low level feature
we see if it's generally useful and not something that is Cilla specific and if that's the case
then we put it in C-Star so that others can benefit and if it's something that's really
Cilla specific we keep it in CillaDB.
Great.
So are there any other projects where C-Star framework is being used apart from Cilla?
Oh there are several such projects.
One is a pedis which is parable or redis, a redis implementation on top of C-Star and
another is Ceph, the distributed file system that is being ported to use C-Star.
Ceph has seen the same problems that many other storage intensive applications see with
the law contention and bad utilization and they recognize that the CillaDB is targeted
at exactly the same kind of application that they're using so they've started the process
of porting Ceph to CillaDB and they've also contributed some components to C-Star in
order to make that transition easier and those are components that are aimed at having a mixed
mode application where some part of the application is still not ported to ThreadPerCore mode
and other parts are and they contributed the glue logic that allows it to build such an
application.
Great.
So, one of the other fundamental changes with CillaDB has done is using the C-Star's
user space disk IOS Scheduler as against the stock Linux kernel Scheduler.
I was interested in knowing the motivation behind it, why we briefly discussed about
it in the earlier part of the show, can you elaborate on that?
Yeah, so again the words that comes up is control.
Then the best place to queue your IOS is in the device itself and lacking that in the
kernel and queuing in user space is actually not the best place but it is a place that
gives you control.
When you queue your request in the device it allows the device to make optimizations
and schedule the IOS according to the way that gives the best throughput but it also
means that when the device or the kernel re-orders those requests it will put some requests before
others and those may not be the requests that you want.
And in a database you have some very intensive IOS going on all the time and those are very
different types of IOS.
So you may have a compaction job running which is needed for database maintenance and at
the same time you have reads that are needed to service cache misses.
And if you let the kernel or the device schedule those IOS for you then you will very likely
see that the compactions dominate and those reads start seeing a high latency.
By queuing in user space we get to pick which is a request that will get serviced next.
So instead of giving everything to the kernel and everything to the device we give them
just enough in order to achieve high throughput and by keeping everything else in user space
we get to choose what is the next request that gets processed and by keeping track of the
amount of IOS that was done for queries and for compaction and for all of the other types
of IOS that we do we can schedule them in the way that we want instead of leaving it
to some decision of the device which is not what we want.
So you talked about maintaining Iocues in user space.
How many Iocues does CillaDB maintain in user space?
So there are two levels of Iocues that the CillaDB maintains.
The lower level is the one that we have pair shard.
We have one queue pair shard which contains all of the IOS that is destined to go into
the disk for that shard and in case that the disk does not have high concurrency for example
it might be a smaller set of disk or even a hard describe which has a very low concurrency.
We keep fewer Iocues than there are shards and that allows us to restrict the concurrency
of Iocues into the disk to a level that it can sustain without incurring a lot of latency.
The second level of Iocues that we have is one Iocue service class and pair shard.
So if you have 20 cores on the machine and if we have five service classes then we will
have 100 Iocues on that machine and those service classes are things like query, compaction,
commit log, mem table flushes and repair.
So those are all operations which we want to isolate from each other so no one can dominate
over the rest.
Great.
So the decisions regarding how many queues we are supposed to maintain, is that done automatically
via an IOTuning tool or is it static for every installation of SolarDB?
Yes, so like I mentioned before we try to make the database autonomous and not push decisions
to the user which might not be an expert in that.
So we have a tool that's called the IOTune and what IOTune does is it runs a benchmark
on the disk and it measures the read bandwidth, the write bandwidth and the read and write
Io operations per second and those Io operations per second are the effective concurrency of
the disk.
So it makes a decision on how many Iocues to allocate for that disk based on that concurrency.
So if the disk is highly concurrent then we have one Iocue per shard and if the disk
has low concurrency then we will have a smaller number of Iocues perhaps even just one Iocue
which all the rest will communicate with.
So moving on, SolarDB extends the shared nothing philosophy to networking too by using
C-stars user space TCP IP stack.
So before we dive into C-stars native networking can you tell us the advantages of using a
user space TCP IP stack over something like NFSD which runs in the kernel.
So first of all C-star does support a user space TCP IP stack.
SolarDB does not use it per default so it's available and you can enable it but it's something
that we do not recommend for production it's mostly for experimenting and the reason is
that it is hard to productize it.
Using a user space TCP stack is difficult because you need to detach the network interface
from the kernel and assign it to the application you need to make sure that you have the correct
IOMU so it involves more work from the user and we decided that the incremental benefit
and performance was not worth it.
So we will enable it in the future but for now we are using the kernel TCP IP stack.
I see.
So this is kind of different than what I was researching on SolarDB on the website.
So I am not sure if this is a recent change which was made in Silla because of the reasons
which you mentioned or was Silla always without a user space TCP IP stack.
So Silla always ships with both and it's always possible and we know that some users
do it but we do not enable the user space TCP IP stack by default simply because it's
hard to do so there's a lot of manual changes that you need to make in order to enable that.
Okay so does Silla still maintain the notion of one TCP IP stack per CPU core or is that
TCP IP stack shared across all the cores?
So only partially when you use the user space TCP IP stack then of course it's completely
like that but when you use the POSIX TCP IP stack it's only partially.
We have a connection per core so that you don't need to make a core hop when you're sending
messages to other nodes you can send them directly and also we configure the kernel
TCP IP stack in a way that reduces the amount of core hops that it needs to make.
Otherwise it's not possible so in those cases we allocate a core just for networking and
that core has the sole role of performing TCP IP that's not ideal of course but it was
the best compromise that we could make.
I see.
So since Silla support both user space and the kernel TCP IP stack can you explain for
the benefit of the listeners where would I want to use one over another in a general
more general context not limited to Silla but if I am a user of a C star framework and
I'm making a decision that do I want to integrate their TCP IP stack in my application what
factors should I consider before I do it?
So actually Silla would also be a good place to use the user space TCP IP stack and the
reason we did not was programmatic we would like to do that one day and the places to
use the user space stack is when you have a high packet rates so the overhead of processing
a packet in the kernel and distributing it to the correct core is much higher than it
is in user space when you can make sure that the packet arrives at the correct core in the
first place.
So if you have a high packet rate application then that's a good use for the user space
TCP IP stack of course it also needs to be amenable to threat per core not all applications
are like that but those that are will benefit from it greatly.
Great.
Another fundamental move away from Cassandra from SillaDB is ditching the Linux kernel
page cache in favor of the user space row based cache which is entirely managed by SillaDB.
Can you explain to the users to the listeners why we did that?
So there were several reasons of course the theme of control also applies here when you
using the Linux page cache then you're basically giving control over how the cache is managed
how things are evicted when it's filled to the kernel and while the kernel is a great
general purpose system it's not tuned to what we want we can make better decisions because
we know our workload.
So for example when we are doing a compaction we don't need any caching we know that the
data is much larger than memory so there's no point in caching it and we know that the
data that we're going to write to disk will not be used in the near future so again there's
no point in caching and by bypassing the cache we bypass all of the work that went into caching
and into managing that cache.
We also by having an object cache instead of a page cache we don't only cache the data
that we've just read from disk but we also cache the work that went into merging the
data from multiple SS tables and the work that went into parsing the data into an object
format so we don't only save IO when we cache we also save CPU and we also save extra memory
because we've merged multiple sources of information and through our ways the duplicates.
So having an object cache allows us to get better memory utilization and reduce our CPU
utilization and of course for an object cache you need something specialized you can't
use the page cache for that.
I see.
So what this practice of because the general theme I'm getting over here is moving some
functionalities from kernel to user space and similar DB has already done that with the
task scheduler and with the cache.
So is that a pattern which you see is going to become popular going ahead in future where
more and more kernel functionalities are coming into user space giving rise to the microkernel
notion which is which we have heard of in the past.
So I think I think we will see it more and more but in specialized applications where
you have requirements for very high throughput if it's something that's going to be used on
a small number of nodes like some kind of in-house application it's probably not worth
the effort just use a few more nodes and save yourself the work but an application like
a database which or a file system which is infrastructural it's going to run on thousands
upon thousands of nodes not all in one user but it's going to run in a large number of
places the effort that it takes to specialize all of those algorithms and use them for the
application is well worth it because it's amortized over a large number of installations.
And because the machines are getting larger wider and wider and having more and more cores
the payoff becomes larger and larger.
So coming back to the Robey's cache which we were discussing why I did have one more
question around that which was regarding the bookkeeping which is required for maintaining
a cache.
So what are the cache eviction and cache environment strategies with CillaDB has since it's now
maintaining all the cache in the user space by itself.
So the cache eviction is a simple error you'll we have plans to make it to use a fancier
algorithm in the future but we haven't had the time to implement them we know that having
an algorithm that takes into account a frequency of use and object size can deliver significant
benefits but we haven't had the time for that.
In terms of invalidation so when we flush a mem table to disk after that we have to do
something with the object in the mem table and what we do is one of two things if the
partition that we are flushing is also exists in the cache then we merge the data into that
cache and also if we know that the partition is new and doesn't exist on disk we also merge
it into the cache and if the partition exists on disk but does not exist in the cache then
we cannot merge it into the cache since the cache is supposed to be a source of truth
and therefore we just throw it away.
So we have those two policies that we apply according to the situation that we have.
Great so how does Celadiby handle the multiple version concurrency control with row based
cache in the mix does it require any special handling?
So yes so mvcc multivergent concurrency control is one of the more complex part of Celadiby
instead of having just one version of a particular piece of data that is indexed by a primary
key we can have multiple versions of that data and whenever there is a write if there
is a read that is running concurrency instead of just updating the cache we create a new
version and this way the write does not affect the ongoing read.
This allows applications to read a consistent version of their data in most current use
cases this doesn't really matter because usually you don't have that high consistency requirement
but we decided to build an mvcc implementation for future transactional use cases where it's
a lot more important.
So moving on now that we have discussed all the architectural differences between Celadiby
and Cassandra let's discuss the routing of read and write request in Celadiby so that
we can touch upon each of the layers in the stack.
So can you describe for the listener how a read and write request will go through the
whole Celadiby stack?
Yes so when the client issues a request it issues the request to a coordinator and now
every Celadiby node has two roles one of them is a role of a replica and the other is a
node of a coordinator and the coordinator is a client facing component.
So if the client is using a token or a driver it will select a coordinator that is also acting
as a replica so it's what a coordinator that happens to be on the same node as a replica
for that data and that saves it in a network of.
So the driver will send a request to the coordinator.
The coordinator will select the replicas that participate in that request hopefully the
coordinator itself will be one of them and it will then send request to those replicas
the number of replicas varies depending on whether it's a read or a write in a write
all of the replicas would participate in a read perhaps fewer and it will send messages
across the network to those replicas.
If you're using a driver that's supplied by Celadiby then the request will also arrive
at the correct core so it will not have to make an extra hop within a node to the right
core and so you will get even better a few put in latency.
I see and is the same part also followed for a write request?
Yes the reads and writes are similar.
The main difference is in the number of replicas that are contacted for writes always all of
the nodes are contacted all of the nodes that are up in order to make sure that all of the
replicas have are up to date.
For reads you may request that only a single replica be contacted so obviously this is not
as consistent but it delivers the best throughput and latency or you may request a quorum and
this is a trade-off between consistency and throughput so you get very good consistency
but you get to reduce the throughput compared to contacting just one replica or you can
even contact all of the replicas for that role in which case you will get the best consistency
but you may suffer from availability in case a node is down the read might not be able
to reach all three replicas and the request will not succeed.
Great so one thing which Cassandra and Celadiby have in common is the gossip protocol for
membership changes in the cluster.
One question which I was thinking about is the interoperability of this membership protocol.
Basically can I have a cluster which has a few Celadiby nodes and a few Cassandra nodes?
Is that possible?
That's a common question but in fact it's not possible while the protocols are very similar
they are not wire compatible and we decided not to go for wire compatibility for two reasons.
One of them is that it would restrict our freedom of implementation and we wanted to
have more freedom in the wire protocol and the other was that we figured that many users
would not like to integrate Celad into a Cassandra cluster because then it might destabilize
the Cassandra cluster and they might not want to do that while they are migrating.
So the typical migration path is to have two parallel clusters running and the client would
send each write into both clusters in order to make sure that the data is consistent and
at some point in time when you are confident and everything is working as expected you
switch over to Celad and continue with just one cluster.
So this is the migration strategy if an enterprise wants to move from Cassandra to the Celadiby.
Indeed and we've seen it done many times with zero migrations with zero downtime you have
exactly the same data on both clusters and you have a period of time during which you
can compare the data and make sure that everything works as expected before you make the decision.
Great.
Celadiby have a notion of transactions because I know that Cassandra claims to have the lightweight
transaction which are a compare and replace operations.
Does Celadiby have the same guarantees or does it provide more complicated transaction
strategies?
All the lightweight transactions is one of the features which are still being implemented
so it's not available yet we hope to have it available soon and in fact we do plan to
expand it once it's available into multi-key transactions we know those are very useful
and we also want to use transactions internally for things like cluster management so instead
of having kind of eventual consistency model when you add or remove nodes do that as a
transaction and also we would like to base our materialist views feature which already
exists and it's shipping we would like to base that on transactions since it simplifies
the implementation by quite a lot and it will also improve performance so we plan to have
extensive use of transactions in Celadiby.
Great.
Can you briefly describe what a lightweight transaction is and how is Celadiby implementing
them in the future release?
Yes so we plan to use the Raft protocol instead of the older Paxos protocol in order to establish
a leader for every group of partitions and that leader will coordinate all of the rights
that go into a particular group of partitions and this way it will be able to prevent concurrent
rights to the same partitions and we order them in a way that is consistent across all
the readers.
I see.
So moving on Celadiby claims to be the drop in replacement of Cassandra.
This would imply an application level API compatibility and a complete support for the
Cassandra query language.
Is that always true or there are places where the two diverge?
So we try not to diverge sometimes we add extensions but we try to make sure that they
don't conflict with the Cassandra language in any way and of course there are sometimes
gaps when we haven't completed a feature and one example is the lightweight transactions.
But these are always things that are just in progress and not some things that we're implementing
in a different way on the client level.
So it's not just the language that is the same it's also the protocol and so you can
use a driver the same driver that you've always used and the application will work just the
same and most of the times when we have an application migrating from Cassandra to Cela
it needs zero changes it just starts working with the higher throughput and lower latency.
So like Cassandra Celadiby has a support of constructing multi data center rings to provide
higher availability SLA guarantees.
However, what features does Celadiby provide for live backup and restore in case of a region
failure if for example the customer is hosting a Celadiby cluster on AWS?
Yes, so there are several features that increase availability.
One of them is support for availability zones so Cela can make sure that there is one replica
for a given piece of data on each availability zone.
So if it's just an availability zone that dies then you can recover the data from the
two surviving availability zones.
Another feature is multiple data center so if you have a region or a data center drop
you can rebuild it from other data centers that have survived and of course for a complete
disaster you have a backup and restore so Celadiby supports the snapshots you can snapshot
the database at any point and you get the set of files that you can then copy to an offline
storage location from which you can reconstruct the database as it was at the point of backup.
So this is just for the clarity this is not the live backup this is more of reconstructing
the data from enough and of and a touch storage which could potentially require some downtime
for the application.
Am I right?
Yes that's correct for for live backup you simply create another data center that is
used for that is being replicated to and that other data center will contain all of the
data that your database has and you can also switch your applications to that and in fact
most deployments behave like that it's it's a multi master database so you can have recent
rights to all of the data centers in parallel you can have your applications run only on
one data center and switch over to another data center in the case of disaster or if
you design your applications you can also have them running on all data centers all
of the time and just increase the number of instances in case you have a reduction of
capability in one of the data centers.
So are these two data centers and that centers in continuous sync of each other are they
synchronizing continuously in an asynchronous fashion or how how far behind one data center
is as compared to the other data center when replicating rights.
So both modes are supported if you choose you can have synchronous replication so that
when you issue a right to the database it will wait until the other data centers have
been fully synchronized and that gives you a high guarantee of consistency but it trades
off latency so you will have to wait for the round trip to those other data centers.
The other option is have asynchronous replication in which case you get the response only from
the local replicas and still will continue replication in the background typically it's
just a few hundred milliseconds delay just the time takes for the data to propagate and
Silla also tries to make tries very hard to make sure that the data is replicated by writing
the data into local disk and trying to replicate it later in case it fails if you have a network
failure and also there are facilities like repair which allows you to re-synchronize
in case you had a loss of connection between between two data centers but in general the
database is fully synchronized at all time.
So just for the clarity the scenario where you described about asynchronous replication
and also asynchronous replication offering are you talking about replication across two
different SillaDB clusters or are you talking about replication between a replica copy for
a key space within a single SillaDB cluster?
It's a single cluster that is spread across multiple data centers.
The database is data center aware and so are the client drivers so you can ask the client
drivers to only access the local data center in order to reduce latency and the other data
centers are just being replicated too and since multi-master you can also have applications
in other data centers accessing their local data centers the nodes in the local data centers
and reading and writing the same data.
Great.
So one last question which I have are the upcoming features in SillaDB.
Can you briefly tell us what are you working on and when can we expect them to be released?
Yes, so two features which we've worked on for a while and will be released in three
that all are support for a new SillaDB format also compatible with Cassandra and that brings
improved support for large partitions so queries within large partitions will be faster and
also the amount of storage that is used will be lower so less storage overhead.
The other feature which we've been working on for a long while is materialist views so
this has been available in experimental mode for a long while but in 3.0 it will be available
for production and other features that we are working on lightweight transactions like
I mentioned we also hope to bring tiered storage so that you could use different types of
storage in the same node you will have expensive and fast storage as a fastier and less expensive
and slower storage for your slow tier an example of that can be the NVMA disks that are available
on Amazon instances and EBS disks which are slower but less expensive and by using a mix
of fast and slow storage you can reduce your storage costs while still providing a high
performance.
Great.
Do you plan to track the Cassandra features in the future or branch off and become something
completely different?
In general we do plan to continue providing the Cassandra features.
The Cassandra ecosystem is great there are lots of drivers for all languages and other
ecosystem participants like Spark and Presto that work with Cassandra so Cassandra compatibility
has been good for us.
We will also do our own things we are not restricted to just doing Cassandra compatibility so we
will bring out one features as well but in general we do plan to bring all of the good
Cassandra features to server DB.
Great.
So are there any questions I should have asked or something you would like to say to our
audience?
I think not I think that you've asked really good questions.
I think that one thing I can say is that you can try SillaDB using the test drive system
that we have which spins up a cluster for an hour and that allows you to see CLDB in
action.
It's really great to go in there into a shell into a shell on one of the nodes and look
at how all the cores are utilized and look at the disk utilization and throughput it's
great to see the disk running at multiple gigabytes per second and all of the cores at
100% for me it's always a thrill to see every cycle being squeezed from those machines.
Right.
So how can people follow you?
Oh so you can follow me on Twitter I might be with the sarcastic sometimes but they also
give some good information.
You can follow the SillaDB blog where I write some blog posts and we have a great technical
blog so which not only covers new features and things about how to use Silla but also
the decisions behind Silla and more low level stuff so things like what we've been discussing
in this podcast.
So if what we talked about is interesting to you I really recommend following the SillaDB
post the user and the technical blog.
So V thank you for coming on the show.
Oh it was my pleasure.
This is Nishant Sunasia for Software Engineering Radio thank you for listening.
Thanks for listening to SE Radio an educational program brought to you by IEEE Software magazine.
For more about the podcast including other episodes visit our website at se-radio.net.
To provide feedback you can comment on each episode on the website or reach us on LinkedIn,
Facebook, Twitter or through our Slack channel at se-radio.slack.com.
You can also email us at team at se-radio.net.
This and all other episodes of SE Radio is licensed under Creative Commons license 2.5.
Thanks for listening.
