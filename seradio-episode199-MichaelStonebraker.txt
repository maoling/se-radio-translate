Link: https://www.se-radio.net/2013/12/episode-199-michael-stonebraker/



This is Software Engineering Radio, the podcast for professional developers on the web at
sce-radio.net.
SC Radio brings you relevant and detailed discussions of software engineering topics
at least once a month.
SC Radio was brought to you by IEEE Software Magazine, online at computer.org slash software.
This is Robert for Software Engineering Radio.
I am joined on the show today by Dr. Michael Stonebreaker.



Dr. Stonebreaker is one of the leading academic researchers in the field of database technology.
He earned his doctoral degree from the University of Michigan.
He was a professor at the University of California Berkeley for over 25 years and is now a professor
at the Massachusetts Institute of Technology.
Dr. Stonebreaker is the author of scores of published research papers in academic journals
and is a recipient of numerous awards including the IEEE Vinoyman Award.
Dr. Stonebreaker is also one of the leading entrepreneurs in the commercialization of
database technology.
His founded six venture backed startups is the architect of the Postgres and Ingers relational
databases and is the co-founder and CTO of VoltDB, the industry sponsor of the open source
VoltDB database.
Dr. Stonebreaker, welcome to Software Engineering Radio.
Thank you very much, Robert.
Dr. Stonebreaker, please tell the listeners more about your background.
Well, actually I started nine venture capital backed companies, so your six numbers a little
low.
As you said, I was on the faculty at the University of California at Berkeley for many years.
And in 1999, we switched my wife and I switched coasts and I've been at MIT since that time.
I am the main architect of the Ingress database system, the Postgres database system, a system
called C-Store which turned into the Vertica database system now owned by Hewlett Packard.
H-Store which turned into VoltDB which you mentioned earlier.
And I'm working on a variety of new database projects and other non-database stuff.
So I'm alive and well.


Your paper, one size fits all an idea whose time has come and gone.
You describe how the database market is segmented.
Please summarize your views on how the database market is segmented.
In that paper which I wrote, I believe in 2006, seven years ago, the basic point I was
trying to make was that the hegemony of relational database systems which is during the 90s,
one size fits all was the answer.


If you had a database problem, all the major database vendors would say Oracle 9i or 11j,
whatever the current version was, is the answer now what was the question?
So the whole thinking was that the traditional architecture which is espoused by Oracle SQL
server and IBM's DB2.
That was the answer to all database questions.
And in 2006, I was trying to make the statement that that wasn't true anymore.
In 2013, I'd like to make a much, much, much stronger statement which is that the traditional
architecture is not good at anything.
And it's totally obsolete technology in the same way that IMS and out of base and IDMS
systems from the 70s are obsolete technology.
And in every market segment I can think of, there is a much, much better solution either
in terms of functionality or performance.
And so I view the database market at a tipping point which is the major architecture being
sold by the vendors is just plain wrong.


And newer and better stuff is either here or just around the corner.
And that users, well, the database vendors either are going to lose market share or have
to somehow pirouette from what they're currently doing to completely different software.
And users are going to face the same kinds of transition pains.


Now the reason I make the statement is I would categorize the database market, the entire
database market into three pieces.
The first one I would call the data warehouse market.
Every major company has a historical data store of customer facing data that they use
business intelligence tools to access.
So that's about round numbers, a third.
A third of the database market I would characterize as OLTP, transaction processing.
That is both the traditional transaction processing that we've known for years, as well as all
kinds of new TP issues.
Things like maintaining the state of massively multiplayer into Internet games is a big transaction
processing problem.
You need to be able to move and shoot at the same time.
And you and 10,000 other people are inhabiting the same geographical state.


Putting ads in real time on Internet pages is another transaction processing problem.
So the Internet and Internet application springs a complete new realm to TP applications.
In addition, we are in the process of sensor tagging everything of significance.
That includes marathon runners so that we can make sure that nobody cheats and goes and
hops on the subway as Rosie Ruiz did in Boston Marathon quite a few years ago.
And MIT is thinking about sensor tagging every library book.
They also are thinking of sensor tagging every physical repair that needs doing on their
physical plant.
So these sensors generate mountainous real time feeds of data.
And processing those feeds is another OLTP problem.
So OLTP both new and old is another sort of third of the market.
The remaining third is a potpourri of other stuff.
And that includes no sequel systems of which I guess Mongo and Couch are probably the dominant
vendors at this point.
Graph database systems such as Neo4j array database systems such as CIDB, Hadoop, and
its various instantiations.
And so it's a third data warehouse is a third OLTP and a third everything else.
Now the everything else piece is not a traditional relational database market at all.
Which is the relational vendors have no market share in that one third.
Even data warehouses is crystal clear at this point that the entire market is going to move
to column stores such as typified by HANA and Virica and ParaXL systems like that.


Even Teradata is rumored to be rewriting their DBMS to be a column store.
So row stores are not competitive in that market and are going to get replaced by column
stores.
And that transition is well along and happening.
And the reason is simple column stores are a factor of 50 or 100 faster than row stores
on the traditional data warehouse problems.
So the traditional architecture doesn't work on data warehouses and is going to get replaced
by something that's 50 or 100 times faster.
That leaves the online transaction processing market.
In the OLTP space, what's occurring is that the size of OLTP databases is not that gigantic.
It basically goes up at the rate that you sell more stuff.
On the other hand, main memory is dropping in price like a rock.
And main memory is dropping faster than OLTP database sizes that are going up.
So OLTP is in the process of becoming a main memory database market and not a traditional
architecture market.
That's already being seen in products such as HANA from SAP and Hecaton from Microsoft,
as well as systems like voltDB, MEMS equals SQL FIRE, things like that.
So OLTP will be a main memory database market.
Wearhouses will be a column store market and everything else is not a traditional architecture
market at all.
So there's no place that I can see that the traditional architecture is viable.
So I see that it will gradually fade into the sunset in the same way that IMS and not a
base are fading into the sunset.
You've told us that the traditional database architecture is on its way out.
Can you summarize for us the main characteristics of the traditional row-based architecture?
Certainly.
I should have done that at the beginning.
So the traditional architecture is exemplified by DB2 SQL Server, Oracle, Postgres, Ingress,
especially all the relational database systems written more than a decade ago.
All those systems have in common is they store records on the disk, meaning the next thing
in disk storage is the next attribute in the same record.
So they store data on the disk row by row by row or record by record by record.
Disk blocks are substantially encoded.
It's vendor-specific exactly how that's done.
But all systems heavily encode disk blocks.
For example, records are stored with a record ID at the front of the record.
There's usually a bitmap which indicates the non-null fields.
When the fixed-length fields are stored, followed by the variable-length fields, the
record structure is very disk-like.
In main memory, there is a buffer pool of disk blocks.
So worthy disk blocks sit in main memory in disk block format.
The standard wisdom is to use record level locking to deal with concurrency control.
And the traditional wisdom is to use a write-ahead log to deal with crash recovery.
So write-ahead log, dynamic row level locking is the way so-called asset is done.
The trees are omnipresent as the indexing structure, and SQL is the language of choice.
There's a query optimizer and a query executor, both of which are row oriented.
And that architecture describes perfectly all the traditional relational database systems.
And at this level, they all look the same.
So when I say the traditional architecture, I mean that architecture.
This was first promulgated by System R, which was the predecessor of DB2 back in the 70s,
and has been used essentially intact ever since.
Dr. Stonebreaker, at a talk you gave here in San Francisco, you demonstrated the results
of some of your research, which showed that databases spend about 90% of their time doing
what could be described as overhead and only 10% on the actual value-producing activity
of reading and writing data.
Please describe the results of your research.
As I said, if you have an OLTP database, chances are it fits in main memory.
And even if it doesn't, we can turn to dealing with that in a minute.
If it fits in main memory and you're using the traditional architecture, then all of
your data sits in the buffer pool, in main memory.
So we ran TPCC, this was, I think, in 2009, which is the standard OLTP benchmark on the
shore DBMS, which is an open-source traditional architecture system.
We use shore only because we had the source code, and Dave Dewitt, who was visiting MIT
at the time, wrote it, so he wanted to use it, of course.
So we, but it has all of the elements of the traditional architecture, and if we had profiled
MySQL or Postgres, they would have looked very similar.
So all the data is in the buffer pool.
We ran TPCC, and we instrumented the code to say, where does all the time go?
And so in round numbers, what we found out was that less than 10% of the path length
goes to doing useful work, meaning actually finding the record and updating it or whatever
else.
The remaining 90% is all overhead.
So the overhead, roughly speaking, divided into four equal-sized pieces of pie.
Of course, they weren't all equal-sized, but they were all significant-sized.
The first one was the overhead of dealing with the buffer pool.
As you can imagine, you have to pick a record out of the buffer pool.
You have to find a record in the buffer pool, and then you have to convert it from its encoded,
disk-oriented format into main memory format so you can operate on it.
So you have to convert it into main memory format, and then if you updated, convert it
back out of main memory format.
In addition, you have to maintain a least recently used list of buffer pool blocks so
that you can evict the right one at the right time.
You have to pin buffer pool blocks when you have an active pointer pointing into it.
All of that stuff in round numbers is one of the four pieces of pie.
The second piece of pie is every time you touch a record, you have to set a readlock.
Every time you update that record, you have to upgrade that lock to a write lock.
Setting locks and maintaining the lock table, looking for deadlocks in the lock table.
Locking is another of the four pieces of pie.
The third piece of pie is every time you write a record, you have to write its before image
and its after image into a write ahead lock.
So the runtime cost of crash recovery was another of the pieces of pie.
The fourth piece of pie, you have to think about for a minute.
And so fourth piece of pie was multi-threading overhead.
All of the major database systems are multi-threaded.
Back in the 80s, when these decisions were made, multi-threading was an awfully good idea
because if you did a read and the record you want was an in main memory, you'd have to
go fetch it off the disk.
You'd block, you'd want to switch to doing something else.
And so it was very useful to have other threads around that could run.
And this was completely to optimize the then prevalent architecture.
These days, we all of a sudden have a lot more cores sharing the same memory.
And database systems all have shared data structures.
The most prevalent one is B-trees.
So if we have 10 threads alive doing useful work in a database system, all sharing main
memory all sharing the same database, then you have 10 threads that may well be traversing
the same B-tree at the same time.
If half of them are doing inserts, you've got to make sure that you don't corrupt that
shared data structure.
So to avoid corruption, there are a variety of techniques, but most of them involve latching
B-tree blocks that may well be updated to keep other updaters from screwing them up.
So the latching and associated thread weights that are associated with shared data structures
is another piece of the pie.
B-trees are a shared data structure.
The lock table is a shared data structure, resource management, data structures are also
shared.
Temporary storage has to be mallec-tied.
So there's just everywhere in database systems that are shared data structures.
So that's the other 90% of the path length in a traditional database system when operating
on data that is in the buffer pool.
So two things become completely obvious.
The first one is that if you have some fancy replacement for B-trees that is faster than
B-trees, and there are dozens of candidate data structures that allow you to do searches
of the B-tree variety.
A better B-tree affects only 10% of the path length, meaning it's not worth doing.
So if you're going to make a difference, you've got to worry about overhead and worrying
about the actual useful work is just not worth it.
So worry about where the cycles go.
The second thing is if you have a database system and let's say it implements data in
main memory, that gets rid of the buffer pool, that slice of the pie is gone.
But if you continue to be multi-threaded, do a right-ahead log and do dynamic record
level locking, then those three slices of the pie are still there.
So if you take any one slice out of this pie and say how much faster do I go, the answer
is well, you go a little bit faster, but it's a marginal win.
Clearly, all of those overhead factors were introduced because they were thought to be
necessary, how can you build a database which provides acid without any of those overhead
sources?
Okay.
So there's been considerable research and a variety of products in the last five or
six years that are all attempting to do much better in this regard.
So I'll just go through roughly what the ideas are.
Number one, you cannot do record level locking.
It is just too expensive.
In all of the new main memory-oriented database systems, none of them use record level locking.
So the two other techniques that are popular, some systems use what's called multi-version
concurrency control, which is an optimistic concurrency control system.
Some of them, including voltDB, use timestamp ordering.
So you've got to use something else, don't even think about record level locking.
So that's the answer to that piece of the pie.
The second piece of the pie has to do with crash recovery.
A fair number of systems continue to use traditional right-of-head logging.
That's going to seriously impair their ability to go fast.
That isn't what voltDB does.
And so, voltDB, instead of doing data logging, which is if you update 10 records, then you
write the before image and after image of those six records into a log and you have to
push that log to disk.
If instead you simply log the command that was being run.
So if you drive up to McDonald's and say I have a number six with fries and a diet
Coke, if you're just logging the command, you just log six fries diet Coke instead of
all the data effects of that transaction.
So volt does what's called command logging or transaction logging, which is wildly more
efficient at run time than data logging.
So it shrinks that piece of the pie down to a sliver by simply doing logical logging at
a much, much higher level.
Now of course, there's no free lunch here.
Somehow there's got to be a cost to command logging and there is, which is if you need
to roll forward from a crash, then you can roll forward a data log way faster than you
can redo the commands that actually produce those data effects.
So recovery time is a lot slower.
On the other hand, essentially all modern OLTP systems use replication to produce high
availability.
So the only time you actually go down is if you have a cluster wide disaster like a power
failure.
So command logging is an efficient engineering trade off, which shrinks that sliver down
to a tiny bit and uses the fact that essentially everybody wants HA and therefore you almost
never have to recover from the command log.
So that's sort of the second piece of the pie and the way volt gets rid of it.
There may well be other ideas, but you've got to shrink the size of the log way way down.
So the third piece of the pie is of course the buffer pool.
All main memory database systems get rid of that completely.
So that's just plain gone.
And so that leaves you with the fourth piece of the pie, which is multi-threading.
And here there are two main ideas.
The first idea, which is the one volt uses is to say, don't multi-thread.
It's too expensive.
So you're about to say, haven't you guys heard that multi-core architectures are a
big thing these days?
And yeah, I've heard that.
So what volt does is if you have a 10-core system that shares 32 gigs of main memory,
we divide up the 32 gigs into 10 pieces, assign each core statically to one of those pieces,
and we treat a multi-core machine as 10 single-core machines, and we run those single-threaded.
So there are no latches.
There is no multi-threading involved.
So that's one solution.
The other solution is to design what are called latch-free data structures.
In other words, shared data structures that don't require latches.
There's quite a bit of research in this area.
And if you're going to move in either of these directions, you have to completely start from
scratch and write a new database system.
The traditional vendors will not be able to use either of these without gigantic rewrites.
So those are kind of the ideas that seem plausible.
And I've mentioned the ones that volt is actually using.
You mentioned two protocols to avoid blocking the timestamp ordering and multi-version
concurrency control.
Could you tell us a little bit about how those work?
Sure.
So let's start with multi-version concurrency control.
So the idea is that I have a transaction, and I'm going to assume that my transaction
is going to succeed.
So I'm just going to go through running, I'm going to execute this whole transaction from
beginning to end.
And I'm going to make a note of everything my transaction reads.
And I'm going to make a note, and I'm of course going to write, whenever I do a write, I'm
going to write the new values.
The key thing here is I'm not going to write them over what used to be there.
I'm going to write them in a new place.
So basically, I go to the end of the transaction, and at the end I note everything I've read,
and everything I've written goes to think of it as a side file.
At the end, I then check to see whether I had any problem with this transaction.
In other words, did anybody write anything that I read that committed before me?
So I basically check at the very end to see whether everything went OK.
And the idea is that in a well-designed database system, 99% of the time transactions succeed,
because otherwise you don't get anything done.
So that's the general idea is that commit time, you go and look and see if there's any
problem.
This is what's called optimistic, which is the complete converse of dynamic locking,
which is pessimistic, which is in dynamic locking, you wait if there might be any problem.
And so you come up against something that might be a problem and you just stop and wait to
see whether it was OK.
And optimistic concurrency control or multi-version concurrency control, you just continue to
the end on the assumption that it isn't going to be a problem.
And I think if your data is on disk, the simulation studies from many, many years ago showed that
dynamic locking worked better.
But I think all the conditions have changed and in main memory, that isn't true.
So that's one possible approach.
The second approach is to order the transactions 1, 2, 3, 4, 5 and make sure that they're done
in perfect order.
And usually that ordering is done using the clock.
And so a transaction just reads the clock and transactions are then executed serially
in clock order and therefore there are no contention problems.
That was the scheme Volt used to use.
They found out that they have a much more sophisticated scheme now that doesn't require
you to synchronize clocks across multiple systems.
And so that requires a blackboard for me to tell you exactly how it works.
But anyway, you either order the transactions 1, 2, 3, 4, 5, either based on a clock or based
on some other mechanism.
Or you use optimistic concurrency control and check at the end if anything bad happened.
You discussed partitioning the data by core.
How does a system like this handle transactions including data that is pinned to different
cores?
Right.
So here again, this is a piece of pragma which is if you're trying to do a million transactions
a second to a one record database, there is simply no way to do it because everybody needs
to update the same thing.
The extension of this is if you have 10 records in a 10 record database and you put one record
on each of 10 nodes and then you try and run a million transactions a second each of which
updates all 10 records.
All hell will break loose because every transaction crosses node boundaries and concurrency control
becomes extremely difficult.
And requires things called two-phase commits and basically just think expensive.
So anyone who does, who has an application that does a lot of distributed transactions,
meaning transactions across chunk boundaries or cross node boundaries, that's fundamentally
expensive.
And so what most everybody figures out one way or another is how to design applications
that where most of the transactions go to a single chunk because otherwise you pay a big
performance penalty.
So the answer is, vault is optimized for the case that most transactions go to a single
chunk and vault is blindingly fast in that case.
We of course continue to do any transactions but the synchronization for multi-chunk transactions
has to be paid.
Use drill down into how vault DB uses multiple nodes to ensure high availability.
Sure.
So 20 years ago, if you went to an OLTP system and executed a transaction, chances are it
was running on a single node machine and if that node crashed, well the whole idea was
you get back up as quickly as you can and you're just down for however long you're down.
And so that was the routine solution which is get back up as quickly as you can and in
the meantime you're just losing revenue.
These days no one I know of is willing to pay down time.
And if Amazon or eBay goes down, they're on the front page of the New York Times.
So anybody who cares is now willing to buy back up hardware and fail over to a back up
copy of the data.
So tandem computers sort of worked all of this out 15 or 20 years ago.
And so basically you run multiple replicas, you have two copies of everything or it's
recopies of everything.
However many copies you want, do you want to run with a belt or a belt and suspenders
or belt suspenders and a rope?
It's your choice how many replicas to run.
And then the idea is the database system makes sure that it implements all the replicas.
It basically does all updates on all replicas.
And if a node goes down, you simply fail over and continue processing using a different
replica.
And voltDB does this.
If there's a failure, you never see it, it's completely masked.
This seems to be table stakes these days and anybody who's serious about OLTP, just no
one is willing to take downtime.
Do the nodes use a distributed consensus protocol to ensure consistency of data?
A great topic, great topic Robert.
So the noSQL guys basically said, well let's just use eventual consistency, which is to
say, let's say there's two copies of Robert's salary, one in San Francisco and one in Boston.
We'll update the one in San Francisco.
We'll give Robert, you know, I did it.
And then we'll propagate that update to the East Coast at our leisure.
And so the idea was that, well, if you just stopped accepting new updates, eventually
the two copies of the database would be the same.
So the noSQL vendors basically promulgated the idea that we'll make the database eventually
consistent, but it isn't consistent in real time.
What this actually means is that the database can be completely trashed.
And so let me give you a quick example.
Let's say that there is exactly one widget remaining in your warehouse.
And so you have two copies of the state of the warehouse.
One in San Francisco, one in Boston.
So let's say that Robert, you log in to San Francisco and you say, oh, there's one widget
and you buy it.
So you now get a commit that you bought the widget.
And so, and at its leisure, the database system sends the, you bought the widget to Boston.
Let's say before it arrives, or let's say there's a network partition or some other bad thing
happens, or even that San Francisco fails.
You now have the Boston database which thinks there's one widget.
And so I log in to the Boston database, see that there's one widget and I buy it.
And I get a commit saying, yep, you bought the widget.
So you now have Robert and Michael both thinking they bought the last widget.
Eventual consistency will say the ultimate state of the database is there's minus one
widget in inventory, which is clearly an unacceptable state.
So eventual consistency doesn't work.
It doesn't work in technical terms.
It doesn't work for any transactions that are not commutative.
It only works if things can be executed in any order and always give the same answer.
And that's rarely true as the widget example shows.
So if you run replicas with eventual consistency, meaning you don't guarantee acid for your
replicas, then you risk garbage the database.
So Volt doesn't do that.
You've got to make sure, in my opinion, the right way to deal with replicas is to say
acid is the answer replicas are in fact consistent.
So there are two ways to do that.
The first way is, and this gets into some gory details.
So I'll try and stay out of the gore.
There are kind of two ways to update replicas.
The first way is you run the command at San Francisco and you run the command at Boston.
So you run the transaction twice once at each replica.
And this is what's called active-active replication.
So that's solution one.
Solution two is you select one site as the primary, say San Francisco.
You run the transaction in San Francisco and then you reliably send the log over the network
to Boston and you roll the log forward.
So that's what's called active-passive.
The primary site is doing all the work and the secondary site is just rolling forward.
So you can do both of these transactionally.
The details get a little bit messy.
But this requires things called two-phase commits.
My strong suspicion is that active-active is way faster than active-passive.
The reason for that is way back to the slices of pie.
You don't want to write a log.
You just don't want to write a log.
And so I'm a big fan of active-active, which is what volt Dp does.
And we make sure that the command is performed at both sites or at neither site.
So I'm a huge fan of consistency.
You've been discussing the way that multiple nodes provide high availability.
Is the system also using multiple nodes to increase throughput?
Of course.
I mean, I think the, again, this I think is table stakes.
If you don't scale, you're nowhere.
Because the standard example is you're an Internet startup and you hit the hockey stick
of growth and you want the database system to be able to expand as your needs expand.
The only way to do that is what's called scale out, which is just add more and more
and more nodes as you need more horsepower.
So I think scale out is the answer.
And high availability just essentially comes for free with scale out.
So any database system that's a single node database system I think will not find success
in the OLTP market.
You've been a critic of some respects of the NoSQL movement.
Could you elaborate on your criticisms?
What's going on in the NoSQL world?
Well, if you look at the primary NoSQL advocate, it's a guy named Jeff Dean at Google.
And Jeff Dean was, has written many papers.
He's the main architect at Bigtable and was one of the principal architects of MapReduce.
So he recently wrote a system called Spanner.
And Spanner, he makes a big point of saying, is a complete asset replica system.
He has gotten the religion.
So Google users have just said, I want consistency, make it happen.
So I think the replica consistency gives users a guarantee about what their data looks like.
Anything else does not.
And I think the NoSQL guys are going to move fairly quickly to joining the asset bandwagon.
The second thing that is happening fairly quickly is that the NoSQL guys all said, NoSQL meant
they wanted to have a low level record of the time language.
And that was what Mongo and Couch and all the other guys started out with.
Very quickly they realized that their users wanted high level utterances.
And both Cassandra and Mongo have specified high level languages to which you can access
their respective databases.
And these two languages, unless you squint, look exactly like SQL.
So I think the NoSQL guys are moving way closer to the SQL camp.
And my sense is the NoSQL guys started out being NoSQL.
Then they turned into not only SQL.
And I think now I would specify them as not yet SQL.
But I think the differences between the NoSQL guys and the SQL guys will diminish dramatically
over time.
Dr. Stonebreaker, you've presented a lot of really interesting ideas about where the database
market has been and where it's going.
If you were to teach undergraduates a database course today, what would you cover?
Yeah, I think this is a really interesting topic because I think all database textbooks
have the traditional wisdom.
And in fact, the MIT database course spends two thirds of the course on the traditional
wisdom.
And that's true of essentially all the other universities I know of.
So we are all teaching the wrong stuff.
And so what I would do is I would teach column stores, I would teach main memory databases.
I would explain what the advantages of graph databases or array database systems are.
I would talk about the NoSQL movement.
You've also been critical of Hadoop, but is there a place for it for the right set of
problems?
Okay, so Hadoop.
Well, let me just be crystal clear what Hadoop means.
Hadoop is really a three level stack.
At the bottom level is HDFS, which is a file system.
On top of that is an open source implementation of map reduce, which is what Hadoop has historically
been called.
So that implements a map and a reduce and a protocol for processing MR style queries.
So this has been well described in the literature and by the Google guys and Hadoop is basically
the open source copy of that.
On top of that, there are a variety of tools of which the most popular are probably Hive
and Pig and Hive and Pig basically think of them as SQL.
So it's a three level stack, sort of SQL, implemented on top of map reduce, on top of
a file system.
So that's the stack.
Now more than 95% of the Facebook workload, and Facebook is a huge user of Hadoop, comes
in through Hive.
So 95% of the traffic is SQL.
5% or less is commands that go directly to the map reduce layer.
So here's what's happening as I see it.
If you run Hive on top of the map reduce layer on top of HTS and you're specifying queries
in Hive, let's just call it SQL.
Then that stack runs wildly slower than any parallel database system.
So you will get horrible performance relative to any of the data warehouse products.
So that's the go slow command by a factor of 50 or 100.
And there's some very, very good technical reasons why that's happening.
And if you open up any of the parallel database systems and I've opened up a bunch of them,
and in fact I wrote one, Vertica, they have nothing that looks like a map reduce layer
in their innards.
That just isn't an interesting interface in a parallel database system.
So 95% of the time you are way better off using something other than the traditional
Hadoop stack.
And in fact, the main proponents of Hadoop, which I'd call Cloudera Hortonworks and Facebook,
all figured this out, and they are all writing or have written Hive implementations that get
rid of the Hadoop layer completely.
So for instance, Cloudera has a system called Impala that's out in the market now.
Impala is an implementation of Hive that does not use the middle piece, the map reduce
the middle piece at all because it's a problem, it's a huge problem.
So the Hadoop proponents are in the process of throwing Hadoop overport.
So I think what will happen is that the 95% of the traffic that comes in through Hive
is going to go through a traditional parallel database like stack, which will have nothing
to do with this map reduce Hadoop layer.
So it will, to a first approximation, vanish.
And that will leave you with two choices.
First number one is run one of the data warehouse products, you know, terra data, vertica,
deta, dot, dot, dot, or run one of the SQL implementations from the Hadoop players like
Cloudera Hortonworks or Facebook.
The Hadoop players will be implementing on top of HDFS, which has some horrible performance
problems of its own.
So I expect the traditional data warehouse vendors will still cream these guys in terms
of performance.
But what will happen is that the Hadoop vendors will start to look like very traditional data
warehouse vendors, so they will move way closer to what looks like a parallel SQL database
system.
So that's 95% of the load.
Five percent of the load is what I would call embarrassingly parallel map reduce style
jobs.
And my best example is if you have a million documents and you want to find all the documents
that have strawberry followed within five words by vanilla, well divide up the million
documents over however many nodes you want to process on, write a Python script that
pause over the documents on each node, do that all in parallel, and then you have a
roll up at the end.
So this is an example of an embarrassingly parallel job.
This runs beautifully in map reduce and will run beautifully in the traditional Hadoop
stack.
But that's five percent or less of your workload.
So if you are a Hadoop enthusiast, make sure that the workload you're trying to run on
the traditional Hadoop stack is in this embarrassingly parallel category.
If it's not, you're in for a world of hurt because you're going to have horrible performance
problems when you go up to, well, if you don't have a problem that looks like this, chances
are you'll be writing stuff in Hive and you're going to have horrible performance problems
when you try and scale.
And so what will happen is all the vendors will be there to pick up the pieces.
All of the data warehouse vendors have, or will soon have, Hive interfaces.
So they can say just shove our parallel database system underneath this lousy stack that you
started out with, the Hadoop vendors will be selling you things like Impala, which are
completely different implementations.
So you will switch out the innards of the traditional open source Hadoop stack and favor
something else.
My advice to you is if you are in the five percent category, you're fine.
You need to use the traditional open source stack.
If you're in the 95 percent, you don't want to go down the Hadoop road at all.
You want to figure out how you're going to scale and where you're going to end up and
go directly there.
Dr. Simrick, I'd like to spend the last part of the interview talking about Volt TV.
Would you tell us a little bit about the company?
So Volt is a startup that I founded five years ago, four years ago.
Its intention was to take the H-store prototype that we wrote at MIT and commercialize it
in the process, robustizing and solidifying the code, adding command logging, adding much
more SQL than was there before, adding dynamic elasticity so you can add resources, i.e.
nodes on the fly without going down.
So there's a long laundry list of things that they have done and others that they are continuing
to do.
The company's about 25 people.
We are very busy.
Sales are great.
There's a huge, huge market for what we do.
The places that we've found early acceptance.
Number one is maintaining the state of internet games.
Things like leaderboards, things like the state of multiplayer games are very high performance
OOTP problems.
We're finding great success there.
We're finding success on Wall Street where people want to, in the midst of electronic
trading platforms, people want high speed OOTP.
Another example where we found acceptance is if you are in financial services and you're
maintaining electronic trading desks all over the world, the CEO of your company wants to
maintain your real time exposure to any given security so that he can limit the cost of
risk to an acceptable amount.
So you want to maintain the real time position worldwide for every security on the planet,
what your company's position is, shorter long than that given security.
That's a high performance OOTP problem.
Those are the places we found early acceptance.
I think we're so-called new SQL systems, which Vult is an example, are crossing the
chasm and finding acceptance in general OOTP problems.
I think the market is ginormous.
Who are some of the other players in the new SQL space?
Sure.
So I think number one, both Hannah and Hannah from SAP is not quite there yet, but it will
be a main memory OOTP system in a year or two.
Hackathon is coming out at the end of this year in SQL Server 14, I think is the next
release.
It's a main memory database system.
I think both of these systems are going to have performance issues that will mean Vult
will run way faster than they do.
Other players are SQL Fire, which is now owned by Pivotal, used to be spun out of VMC, written
by VMware.
So SQL Fire is another contender.
New ODB is a startup here in the Boston area.
Mem SQL is a startup on the west coast.
So those are some of the players.
If I'm not mistaken, VultDB is open source.
Is that right?
Correct.
So I think here I'd like to make a general comment, which is I think until maybe 2007
or 2008, it was essentially impossible to get a venture capitalist to back a system software
company that was doing open source software.
That all changed with Red Hat.
They paved the way for showing that you could make a financially viable proposition with
open source software.
So I think most of the database systems written in the last five or six years are open source.
I'm a huge fan of this business model, and it has nothing to do with being altruistic.
This is all about the sales channel.
So if you have a traditional closed source piece of system software, well, you hire a
bag carrying, meat eating, beer swinging salesman who makes about $250,000 a year at quota.
However, he is an expert at taking the customer to lunch, not in explaining technical nuances
of his product.
So you pair him with what's called a sales engineer who makes $150k at quota.
And so you have what's called a four-legged sales team who with their management and overhead
is making around half a million dollars a year as their cost.
Customers have figured out that once they buy an expensive piece of proprietary software,
they're really hooked.
So their whole idea is to get as much freebies out of the sales team up front as possible.
So they will try and make you write their application if they can, and just generally
squeeze you as much as they can before they sign on the dotted line.
They've also figured out that the software goes on sale the last day of the quarter.
So they wait to negotiate until the last day of the quarter of whatever your quarter is.
And so every proprietary system software company brings in the majority of its quarterly revenue
in the last week of the quarter and has this very, very expensive sales process that consumes
40, 50% of revenue just making the sale.
So it's just a very expensive sales channel that's very unpredictable.
It tends to result in customers getting bad customer support because the meeting, eating
salesmen is on to the next deal the minute he signs you and has no incentive to keep
you happy.
So I've been associated with company after company that has had the sales channel and
I really hate it because it's just a very, very expensive way to make a sale.
Open source works completely differently.
You drive customers to your website, you say download the code, see if you like it.
And you get the customer doing his own sales evaluation rather than you sending out this
four-legged sales team.
So and you don't do free pilots, this allows you to have way, way lower sales costs and
therefore way, way lower prices.
So I think it's a way that everybody wins.
It's just a much more efficient sales channel that results in much lower prices.
If listeners are interested in reading some of your research or learning more about you,
where's the best place for them to go?
The simple answer is if you Google my name, you'll get to a site called Sites here that
has my complete resume with pointers to essentially everything I've ever written.
In addition, I do actually blog for ACM on an irregular basis.
So if you go to the ACM website, there is some blogging information there.
Listeners would like to learn more about VoltDB.
How can they do that?
There's all kinds of information on the website including how to get more information.
Dr. Michael Stonebreaker, thank you very much for speaking to software engineering radio.
This is a lot of fun Robert, thanks and I want to compliment you on the sophistication
of your questions.
Thank you so much.
Thanks for listening to SC Radio, an educational program brought to you by IEEE Software Magazine.
For more information about the podcast including other episodes, visit our website at se-radio.net.
To support us, you can advertise SC Radio by clicking the dig, Reddit, delicious or slash
that buttons on the site or by talking about us on Facebook, Twitter or your own blog.
If you have feedback specific to an episode, please use the commenting feature on the site
so that other listeners can respond to your comments as well.
This and all other episodes of ACM Radio is licensed under the Creative Commons 2.5 license.
Please see the website for details, thanks again for your support.
