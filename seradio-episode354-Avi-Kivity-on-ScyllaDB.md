Link: https://www.se-radio.net/2019/02/se-radio-episode-354-avi-kivity-on-scylladb/

This is Software Engineering Radio, the podcast for professional developers on the web at se-radio.net. SE Radio is brought to you by the IEEE Computer Society, the IEEE Software Magazine. Online at computer.org slash software.



**Nishant Suneja** 00:00:23 Hello, this is Nishant Suneja from Software Engineering Radio.Today I am speaking with Avi Kivity from ScyllaDB. Hi Avi.

**Avi Kivity** 00:00:34 Hi Nishant, glad to be here.

**Nishant Suneja** 00:00:36 Great. Avi was a lead developer for Kumra Net Inc, the company that created the well-known KVM project, the hypervisor underlying many production clouds. Kumra Net Inc was acquired by Red Heart in 2008. But after, Avi worked as a lead developer maintainer of the open-source KVM project until December of 2012. Avi is now the CTO of ScyllaDB, a company that seeks to bring the same kind of innovation to the public cloud space. So Avi, is there anything which I missed which you think is relevant in your past experience?

**Avi Kivity** 00:01:16 I also have had experience in Exynet, which was a company that was building distributed storage, high performance distributed storage. And that's of course relevant to building high performance distributed databases. It's maybe one step down from distributed databases. Definitely.

**Nishant Suneja** 00:01:35 Thanks for filling in that missing gap. So today we will be talking at length about the technology which is powering ScyllaDB. To get started, Avi, can you describe briefly what is ScyllaDB?

**Avi Kivity** 00:01:49 So ScyllaDB is a distributed no-sql database. It's compatible with Cassandra. And the three main characteristics that I would say is first of all the compatibility with Cassandra, which we try to make sure it's very compatible to make sure that it's easy to use and to adopt the high performance, perhaps the most well-known feature, so that you can get the most out of your hardware and you can bring low latency and high throughput to your application. And the third is Autonomous. So ScyllaDB tries to tune itself as much as it can, adapt itself to the workload so that you don't need to keep fiddling with configuration parameters.

**Nishant Suneja** 00:02:36 Great. That's a good summary of at high level what ScyllaDB does. So one thing which I was always curious about is the motivation behind building the ScyllaDB. Perhaps you can give us a high-level view of what you set out to do in building Scylla.

**Avi Kivity** 00:02:55 Yeah, so the origin story of ScyllaDB is that the company was actually doing something else some years ago. We were building OSP, which is a cloud operating system. And one of the workloads that we set out to optimize was Cassandra. We knew it was a popular database and heavily used at many places. So we ran Cassandra on top of OSV and tried to see what performance gains we could bring to it. And what we saw was that there was nothing at the operating system level that we could do to improve Cassandra because it was busy. Most of the time it was busy fighting with itself. Either it was garbage collecting or the threads were busy taking locks or it was using the file system in a way which could not be optimized. So we saw a lot of bad performance characteristics in Cassandra and we did like the architecture. They shared-nothing and fully symmetric architecture. So the combination of good architecture but lacking implementation led us to see that there was an opportunity here. And of course there was something else that led us to that. It was that OSV was not doing so well. So we decided to pivot. And ever since then we've been doing ScyllaDB.

**Nishant Suneja** 00:04:21 So you talked about a few issues which you encountered with Cassandra. Were there any use cases in retrospections which you think that Cassandra is well suited for perhaps over ScyllaDB?

**Avi Kivity** 00:04:39 So of course we tried to make sure that there are no such use cases. We tried to make sure that we were always better than Cassandra. Always making a better utilization of resources and delivering better latency and better manageability. Still there are some cases where there are features at Cassandra which we did not yet implement. So in those cases it might be a good idea to start with Cassandra and perhaps move to ScyllaDB when those features are implemented and when your use cases grown enough that you need the power of ScyllaDB.

**Nishant Suneja** 00:05:18 I see.So basically I would want to use Cassandra if there is a new feature in Cassandra which is not yet ported over to ScyllaDB. Otherwise I should stick with Scylla.

**Avi Kivity** 00:05:33 That's my bias recommendation.

**Nishant Suneja** 00:05:36 So to set the stage and have the listeners get a motivation of ScyllaDB project can you briefly share with us some statistics around throughput and latency comparison of ScyllaDB with Cassandra?

**Avi Kivity** 00:05:50 So we've published several benchmarks that show that you can reach a million operations per node and of course it scales horizontally so as you add more nodes your throughput grows with number of nodes. This is just a general number so of course it depends on the workload if you have complex operations or operations that involve large amounts of or larger rows then you will see lower throughput with lower number of operations per per node if the operations are simpler than you can see higher throughput and of course throughput is not just about operations per second it's also about how the database does its own internal maintenance things like repair, things like bootstrapping and you know so all of these need to be taken into consideration and we have benchmarks and blog post that show all of them.

**Nishant Suneja** 00:06:52 Right I was going through the benchmarks on the ScyllaDB website and I came across the Yahoo cloud serving benchmark and the numbers were pretty impressive. The throughput which ScyllaDB was achieving on a three node cluster Cassandra was achieving on a 30 node cluster with lower P99 latencies for an enterprise this basically means 10 times cost saving so which I personally found very remarkable.

**Avi Kivity** 00:07:26 Yes so of course the less hardware you need then the more savings you get or sometimes conversely with the same amount of hardware you can do more or achieve lower throughput we typically see a mix so people don't scale down the hardware as much as they can in order to get more reserve and to get better latency on their throughput so it's a mix of better latency, better throughput and reduced node count which drive migrations.

**Nishant Suneja** 00:07:59 Great so let's dive in details in each of the fundamentals which make ScyllaDB faster and more scalable than Cassandra. I'm sure our listeners will be interested in knowing what makes ScyllaDB faster. So one primary difference between the two databases is the language in which they are implemented. ScyllaDB in C++ and Cassandra in Java. What was the motivation behind choosing C++ over Java?

**Avi Kivity** 00:08:30 So the number one motivation was control. Java is a managed language it runs in the JVM and the JVM mediates everything that you do with respect to the hardware and the operating system. In contrast C++ allows you to do anything you like. It also allows you to do bad things like accessing the free memory and shooting yourself in the foot in general but it also allows you to do good things so you can lay out data in memory in whichever way you like. You can access any OS function or bypassing OS. You have excellent control. There's also the advantage in general it's faster but that was not the main motivation and it's not a huge advantage. So world written Java code can approach the performance of C++ code but it can never approach the amount of control that C++ gives you.

**Nishant Suneja** 00:09:26 Great.So in the past we have done a few shows where we have discussed garbage collection and it's impact on tail latency of an application. I have referenced them in the show notes. I was interested in hearing from you as to what is the impact of GC pauses on a database like ScyllaDB. If it was implemented in Java in this case Cassandra will be more relevant.

**Avi Kivity** 00:09:56 Yes, so for Cassandra of course garbage collection is one of the main Achilles hills. It's well known. You can see it in the user list and you can hear complaints from users about garbage collection that can pause a note for hundreds of milliseconds at a time and even more. It's usually doing unpredictable things like suddenly you materialize a large partition and it that effort creates a lot of garbage and the database has to pause and stop responding to request. In a distributed environment that can also cause problems that propagate across the cluster because suddenly that node is not responding to other nodes and of course if your client use case is latency sensitive then you feel the impact immediately. You have no response from that node for those hundreds of milliseconds and you cannot continue your workload.

**Nishant Suneja** 00:11:00 So one of the tasks which GC also performs is heap compaction. Without the presence of a garbage collector how does Scylla manage the heap fragmentation?

**Avi Kivity** 00:11:14 That's an excellent question. So many C or C++ databases or in-memory databases don't do compaction and as a result they can reach a problem where you're changing the average size of the data that you store. You can run into heap fragmentation and then when you try to store a larger object you end up having to evict your entire cache in order to find enough contiguous space to store the new data and of course we didn't want that to happen with the ScyllaDB. So we use a combination of methods to avoid that. The most important is that instead of using a regular allocator we have an additional allocator which is called the log structured allocator similar to using a log structured merge tree in storing the data. So the way that the log structured allocator works is when we allocate we always allocate to a new position and when we free we do not immediately reuse the free data and in that case in that regard it's similar to garbage collection. And the log structured allocator is aware of the location and the data types of all of the data that's stored in it. So it is able to move the data whenever it wants and this way it can compact the heap. It uses 128k segments to store this data and that also means that we fragment larger data. So if the user access to store a blob that is a megabyte in size we will spread that blob over many different smaller blobs and of course keep track of them and this gives the allocator flexibility to move those sub fragments around. On demand it can free memory and give us those large 128k segments whenever there is need for a new allocation.

**Nishant Suneja** 00:13:23 Right so when the allocator moves around these fragments doesn't we have the same scenario as Java garbage collection where we will be pausing the existing application threads.

**Avi Kivity** 00:13:40 So first of all the log structure allocator is local to a shard or a thread so it's completely unrelated to the others. And second it's similar to a garbage collector in that it compacts but it's not similar in that we already know all of the free data. We don't need to traverse the live set like a garbage collector that in order to find out what is live and then understand that everything else is a garbage. You already know what the garbage is. It's explicitly free. It's also only user objects that are stored in the log structure allocator. Regular objects that are used as part of a request and responses and for general housekeeping of the database those are allocated using the regular allocator so they don't participate in this. And this allows us to place bounds on the amount of time it takes us to evict things. It's not under control over some general purpose garbage collector which is not aware of our requirements. It's under our control of the log structure allocators control so we know exactly when is the right time to initiate compaction.

**Nishant Suneja** 00:14:58 Great. This was a good discussion on memory management and scylla. So moving on, was there any particular reason for choosing C++14 over more battle tested C++11? What are the differences between them which you think are discussion worthy?

**Avi Kivity** 00:15:19 So C++14 is an incremental update over C++11 but in fact we did have a reason to use C++14 and that is move capture in lambdas. So we use lambdas extensively since we are an asynchronous application and lambdas are generally very useful for asynchronous applications and the ability to use move captures means that we can avoid copying data in continuations and that improves the performance quite a lot. This one feature that causes to use C++14 but by now we have moved to C++17 which has a lot of very nice features but none of them were as compelling. So we could have continued using C++14 but chose to update in order to enjoy the quality of life benefits.

**Nishant Suneja** 00:16:23 So you mentioned about scylla asynchronous model. Let's discuss more about it. So we hear a lot about scylla share-nothing architecture. Before we talk about scylla share-nothing architecture and how is it designed. Let's talk about share-nothing architecture in general. What according to you is a share-nothing architecture when it comes to software engineering and computer architecture?

**Avi Kivity** 00:16:52 So for that we have to look into the past where we have shared something architectures and typically that something was shared storage. So you would have a very expensive storage area network that you can access from multiple computers and before that you had the double-tail SCSI that so you had a disk with multiple interfaces that could connect to multiple SCSI buses and this way was used to implement clusters. So you had a shared pool of storage connected to multiple machines which could each access the storage and that has the advantage of not requiring the users to think about partitioning the data. So no matter how the data was organized you could always fit it on your shared storage pool but it has a disadvantage that storage was relatively exotic and also very expensive. So people moved out of that in order to use commodity storage, much more inexpensive direct to touch storage. It's also much faster because everything is connected directly and doesn't have to go through storage area networks. So the combination of price and performance caused the industry to move away from shared-storage into shared-nothing.

**Nishant Suneja** 00:18:18 So has the shared-nothing architecture become more important in recent times where the per core clock cycles per seconds are plateauing and storage and networking devices are becoming faster? Is that one of the primary reasons which is driving this movement?

**Avi Kivity** 00:18:37 So first of all let's make a distinction. ScyllaDB uses the shared-nothing architecture in two levels. The first level is shared-nothing among nodes and that's the same as Cassandra. You have a number of nodes. Each of them is using its own storage and only communicating via the network instead of using shared storage and this is more unique to ScyllaDB. It also uses shared-nothing architecture among the cores. So each core only accesses its own memory, its own files and usually its own connection connections.So this brings the same advantages. It reduces the need for locking which is expensive in shared architecture and it allows it to use a large number of cores that are available. It looks like the core war has restarted and now we have servers from AMD with 48 cores per chip and I think now even 64 cores per chip. And Intel is also starting launched cores CPUs with 56 cores per chip. So you have very large numbers of cores and trying to write an application that can take advantage of all of those cores with a traditional locking architecture is almost impossible. It is very hard to get that right. Sure. So shared-nothing on a node level is extremely important these days.

**Nishant Suneja** 00:20:12 I see. So I think you briefly talked about it.
But what do you think is a disadvantage of having multiple application threads per core?

**Avi Kivity** 00:20:21 So when you have many application threads then it means that the scheduler, the kernel scheduler has to make the decision about which thread gets to run each time. And of course that means that you lose control. You're moving the decision to a component that is general purpose which of course well designed and tested in many, many places but in something like a database you do need your control. You might hear me say that more than once on this talk. Another problem is of course that in order to mediate access to data you will need locks and locks are becoming more and more expensive and when you transition from one thread to another again you go through the kernel and in these days of a meltdown and specter context switches become more and more expensive. The kernel has to make sure that the state is not leaked every time you transfer to the kernel and back. So those context switches are becoming slower over time instead of becoming faster. So by having exactly one thread per core the kernel does not need to make any decision about which thread to run just always one thread and all of those costs that are involved with switching threads all of those are gone.

**Nishant Suneja** 00:21:48 Great. So in Scylla if two cores do require to communicate we will discuss why that would be necessary but if they do require to communicate how do they do it because locking is something which we don't want. So what is a communication mechanism for application threads where one application is running on each core in ScyllaDB.

**Avi Kivity** 00:22:14 So every pair of cores is connected by a pair of single producers single consumer queues and those are well understood queues that can be implemented without any locks just with the memory barriers. So when one core wants another core to do something on its behalf it places the message into the queue in due time that other core will pull the message out of the queue, perform the operation that was requested and place a response in the return queue. And that may sound like a lot of work but when you apply batching to that then each other operation becomes amortized because when you pull messages from a queue you're actually pulling a lot of messages not just one and that reduces the cost per message by quite a lot and you have really high throughput doing that.

**Nishant Suneja** 00:23:08 So one question which I was wondering is since Scylla is doing everything on a per core basis the database sharding is done on a per core basis and as we will discuss further the network stack is also on a per core basis why would intercore communication be necessary in the first place?

**Avi Kivity** 00:23:30 It's a great question and in fact we are trying to reduce the amount of intercore communication. The most common place where we have that is the client connections. So when the client connects to a node it doesn't know that which sharded is connected to and so it will send requests that are targeted at that node but not at a particular shard and this way we might have a request arriving at one shard but which actually needs to be processed by a different shard or on another core. And that was the main cause for our intercore communications and in fact we added an enhancement to the product call and to the drivers which allow each client driver to connect to generate one connection per core and this way it can send the request directly at the core to service it. So in fact on new RCL implementations you will see much less intercore communications but of course sometimes you are using an older driver and client update so you might still see it. We are working hard to eliminate all of those intercore communications not only reduce performance but they also generate imbalance and imbalance is a pain of thread per core implementations. We like to see the workload evenly balanced among all the cores and all the nodes.

**Nishant Suneja** 00:24:52 Great. So let's talk about the seastar framework. The seastar framework powers a lot of technology behind a ScyllaDB. So can you explain what is seastar framework and what is its relationship with scylla?

**Avi Kivity** 00:25:10 So seastar is a generic framework for thread per core applications. It's oriented at server applications that do a mix of multi-core of IO and networking. So applications that do networking you have many applications these days that are oriented that networking that do asynchronous networking but very few frameworks that do asynchronous IO and that fully utilize a thread per core architecture. So things like distributed databases, distributed file systems or other applications that are storage intensive will benefit from seastar. Obviously it's co-developed with the scylla since whenever we need a new low level feature we see if it's generally useful and not something that is scylla specific and if that's the case then we put it in seastar so that others can benefit and if it's something that's really scylla specific we keep it in ScyllaDB.

**Nishant Suneja** 00:26:10 Great. So are there any other projects where seastar framework is being used apart from scylla?

**Avi Kivity** 00:26:18 Oh there are several such projects. One is a pedis which is parable or redis, a redis implementation on top of seastar and another is Ceph, the distributed file system that is being ported to use seastar. Ceph has seen the same problems that many other storage intensive applications see with the law contention and bad utilization and they recognize that the ScyllaDB is targeted at exactly the same kind of application that they're using so they've started the process of porting Ceph to ScyllaDB and they've also contributed some components to seastar in order to make that transition easier and those are components that are aimed at having a mixed mode application where some part of the application is still not ported to ThreadPerCore mode and other parts are and they contributed the glue logic that allows it to build such an application.

**Nishant Suneja** 00:27:20 Great. So, one of the other fundamental changes with ScyllaDB has done is using the seastar's user space disk IO Scheduler as against the stock Linux kernel Scheduler. I was interested in knowing the motivation behind it, why we briefly discussed about it in the earlier part of the show, can you elaborate on that?

**Avi Kivity** 00:26:18 Yeah, so again the words that comes up is control. Then the best place to queue your IO is in the device itself and lacking that in the kernel and queuing in user space is actually not the best place but it is a place that gives you control. When you queue your request in the device it allows the device to make optimizations and schedule the IOS according to the way that gives the best throughput but it also means that when the device or the kernel re-orders those requests it will put some requests before others and those may not be the requests that you want. And in a database you have some very intensive IO going on all the time and those are very different types of IO. So you may have a compaction job running which is needed for database maintenance and at the same time you have reads that are needed to service cache misses. And if you let the kernel or the device schedule those IOS for you then you will very likely see that the compactions dominate and those reads start seeing a high latency. By queuing in user space we get to pick which is a request that will get serviced next. So instead of giving everything to the kernel and everything to the device we give them just enough in order to achieve high throughput and by keeping everything else in user space we get to choose what is the next request that gets processed and by keeping track of the amount of IOS that was done for queries and for compaction and for all of the other types of IOS that we do we can schedule them in the way that we want instead of leaving it to some decision of the device which is not what we want.

**Nishant Suneja** 00:29:42 So you talked about maintaining IOQS in user space. How many IOQS does ScyllaDB maintain in user space?

**Avi Kivity** 00:29:52 So there are two levels of IOQS that the ScyllaDB maintains. The lower level is the one that we have pair shard. We have one queue pair shard which contains all of the IOS that is destined to go into the disk for that shard and in case that the disk does not have high concurrency for example it might be a smaller set of disk or even a hard describe which has a very low concurrency. We keep fewer IOQS than there are shards and that allows us to restrict the concurrency of IOQS into the disk to a level that it can sustain without incurring a lot of latency. The second level of IOQS that we have is one IOQS service class and pair shard. So if you have 20 cores on the machine and if we have five service classes then we will have 100 IOQS on that machine and those service classes are things like query, compaction, commit log, memtable flushes and repair. So those are all operations which we want to isolate from each other so no one can dominate over the rest.

**Nishant Suneja** 00:31:13 Great. So the decisions regarding how many queues we are supposed to maintain, is that done automatically via an IOTuning tool or is it static for every installation of ScyllaDB?

**Avi Kivity** 00:31:27 Yes, so like I mentioned before we try to make the database autonomous and not push decisions to the user which might not be an expert in that. So we have a tool that's called the IOTune and what IOTune does is it runs a benchmark on the disk and it measures the read bandwidth, the write bandwidth and the read and write IO operations per second and those IO operations per second are the effective concurrency of the disk. So it makes a decision on how many IOQS to allocate for that disk based on that concurrency. So if the disk is highly concurrent then we have one IOQS per shard and if the disk has low concurrency then we will have a smaller number of IOQS perhaps even just one IOQS which all the rest will communicate with.

**Nishant Suneja** 00:32:20 So moving on, ScyllaDB extends the shared-nothing philosophy to networking too by using seastars user space TCP/IP stack. So before we dive into seastars native networking can you tell us the advantages of using a user space TCP/IP stack over something like NFSD which runs in the kernel.

**Avi Kivity** 00:32:43 So first of all seastar does support a user space TCP/IP stack. ScyllaDB does not use it per default so it's available and you can enable it but it's something that we do not recommend for production it's mostly for experimenting and the reason is that it is hard to productize it. Using a user space TCP stack is difficult because you need to detach the network interface from the kernel and assign it to the application you need to make sure that you have the correct IOMU so it involves more work from the user and we decided that the incremental benefit and performance was not worth it. So we will enable it in the future but for now we are using the kernel TCP/IP stack.

**Nishant Suneja** 00:33:29 I see. So this is kind of different than what I was researching on ScyllaDB on the website. So I am not sure if this is a recent change which was made in Scylla because of the reasons which you mentioned or was Scylla always without a user space TCP/IP stack.

**Avi Kivity** 00:33:53 So Scylla always ships with both and it's always possible and we know that some users do it but we do not enable the user space TCP/IP stack by default simply because it's hard to do so there's a lot of manual changes that you need to make in order to enable that.

**Nishant Suneja** 00:34:16 Okay so does Scylla still maintain the notion of one TCP/IP stack per CPU core or is that TCP/IP stack shared across all the cores?

**Avi Kivity** 00:34:30 So only partially when you use the user space TCP/IP stack then of course it's completely like that but when you use the POSIX TCP/IP stack it's only partially. We have a connection per core so that you don't need to make a core hop when you're sending messages to other nodes you can send them directly and also we configure the kernel TCP/IP stack in a way that reduces the amount of core hops that it needs to make. Otherwise it's not possible so in those cases we allocate a core just for networking and that core has the sole role of performing TCP/IP that's not ideal of course but it was the best compromise that we could make.

**Nishant Suneja** 00:35:19 I see. So since Scylla support both user space and the kernel TCP/IP stack can you explain for the benefit of the listeners where would I want to use one over another in a general more general context not limited to Scylla but if I am a user of a seastar framework and I'm making a decision that do I want to integrate their TCP/IP stack in my application what factors should I consider before I do it?

**Avi Kivity** 00:35:52 So actually Scylla would also be a good place to use the user space TCP/IP stack and the reason we did not was programmatic we would like to do that one day and the places to use the user space stack is when you have a high packet rates so the overhead of processing a packet in the kernel and distributing it to the correct core is much higher than it is in user space when you can make sure that the packet arrives at the correct core in the first place. So if you have a high packet rate application then that's a good use for the user space TCP/IP stack, of course it also needs to be amenable to threat per core not all applications are like that but those that are will benefit from it greatly.

**Nishant Suneja** 00:36:42 Great. Another fundamental move away from Cassandra from ScyllaDB is ditching the Linux kernel page cache in favor of the user space row based cache which is entirely managed by ScyllaDB. Can you explain to the users to the listeners why we did that?

**Avi Kivity** 00:37:03 So there were several reasons of course the theme of control also applies here when you using the Linux page cache then you're basically giving control over how the cache is managed how things are evicted when it's filled to the kernel and while the kernel is a great general purpose system it's not tuned to what we want we can make better decisions because we know our workload. So for example when we are doing a compaction we don't need any caching we know that the data is much larger than memory so there's no point in caching it and we know that the data that we're going to write to disk will not be used in the near future so again there's no point in caching and by bypassing the cache we bypass all of the work that went into caching and into managing that cache. We also by having an object cache instead of a page cache we don't only cache the data that we've just read from disk but we also cache the work that went into merging the data from multiple sstables and the work that went into parsing the data into an object format so we don't only save IO when we cache we also save CPU and we also save extra memory because we've merged multiple sources of information and through our ways the duplicates. So having an object cache allows us to get better memory utilization and reduce our CPU utilization and of course for an object cache you need something specialized you can't use the page cache for that.

**Nishant Suneja** 00:38:48 I see. So what this practice of because the general theme I'm getting over here is moving some functionalities from kernel to user space and scyllaDB has already done that with the task scheduler and with the cache. So is that a pattern which you see is going to become popular going ahead in future where more and more kernel functionalities are coming into user space giving rise to the microkernel notion which is which we have heard of in the past.

**Avi Kivity** 00:39:27 So I think I think we will see it more and more but in specialized applications where you have requirements for very high throughput if it's something that's going to be used on a small number of nodes like some kind of in-house application it's probably not worth the effort just use a few more nodes and save yourself the work but an application like a database which or a file system which is infrastructural it's going to run on thousands upon thousands of nodes not all in one user but it's going to run in a large number of places the effort that it takes to specialize all of those algorithms and use them for the application is well worth it because it's amortized over a large number of installations. And because the machines are getting larger wider and wider and having more and more cores the payoff becomes larger and larger.

**Nishant Suneja** 00:40:25 So coming back to the Robey's cache which we were discussing why I did have one more question around that which was regarding the bookkeeping which is required for maintaining a cache. So what are the cache eviction and cache environment strategies with ScyllaDB has since it's now maintaining all the cache in the user space by itself.

**Avi Kivity** 00:40:51 So the cache eviction is a simple error you'll we have plans to make it to use a fancier algorithm in the future but we haven't had the time to implement them we know that having an algorithm that takes into account a frequency of use and object size can deliver significant benefits but we haven't had the time for that. In terms of invalidation so when we flush a memtable to disk after that we have to do something with the object in the memtable and what we do is one of two things if the partition that we are flushing is also exists in the cache then we merge the data into that cache and also if we know that the partition is new and doesn't exist on disk we also merge it into the cache and if the partition exists on disk but does not exist in the cache then we cannot merge it into the cache since the cache is supposed to be a source of truth and therefore we just throw it away.So we have those two policies that we apply according to the situation that we have.

**Nishant Suneja** 00:42:07 Great so how does ScyllaDB handle the multiple version concurrency control with row based cache in the mix, does it require any special handling?

**Avi Kivity** 00:42:20 So yes so mvcc(multi version concurrency control) is one of the more complex part of ScyllaDB instead of having just one version of a particular piece of data that is indexed by a primary key we can have multiple versions of that data and whenever there is a write if there is a read that is running concurrency instead of just updating the cache we create a new version and this way the write does not affect the ongoing read. This allows applications to read a consistent version of their data in most current use cases this doesn't really matter because usually you don't have that high consistency requirement but we decided to build an mvcc implementation for future transactional use cases where it's a lot more important.

**Nishant Suneja** 00:43:16 So moving on. now that we have discussed all the architectural differences between ScyllaDB and Cassandra let's discuss the routing of read and write request in ScyllaDB so that we can touch upon each of the layers in the stack. So can you describe for the listener, how a read and write request will go through the whole ScyllaDB stack?

**Avi Kivity** 00:43:41 Yes so when the client issues a request, it issues the request to a coordinator and now every ScyllaDB node has two roles one of them is a role of a replica and the other is a node of a coordinator and the coordinator is a client facing component. So if the client is using a token or a driver it will select a coordinator that is also acting as a replica so it's what a coordinator that happens to be on the same node as a replica for that data and that saves it in a network of. So the driver will send a request to the coordinator. The coordinator will select the replicas that participate in that request hopefully the coordinator itself will be one of them and it will then send request to those replicas the number of replicas varies depending on whether it's a read or a write in a write all of the replicas would participate in a read perhaps fewer and it will send messages across the network to those replicas. If you're using a driver that's supplied by ScyllaDB then the request will also arrive at the correct core so it will not have to make an extra hop within a node to the right core and so you will get even better a few put in latency.

**Nishant Suneja** 00:45:09 I see and is the same part also followed for a write request?

**Avi Kivity** 00:45:15 Yes the reads and writes are similar. The main difference is in the number of replicas that are contacted for writes always all of the nodes are contacted all of the nodes that are up in order to make sure that all of the replicas have are up to date. For reads you may request that only a single replica be contacted so obviously this is not as consistent but it delivers the best throughput and latency or you may request a quorum and this is a trade-off between consistency and throughput so you get very good consistency but you get to reduce the throughput compared to contacting just one replica or you can even contact all of the replicas for that role in which case you will get the best consistency but you may suffer from availability in case a node is down the read might not be able to reach all three replicas and the request will not succeed.

**Nishant Suneja** 00:46:15 Great so one thing which Cassandra and ScyllaDB have in common is the gossip protocol for membership changes in the cluster. One question which I was thinking about is the interoperability of this membership protocol. Basically can I have a cluster which has a few ScyllaDB nodes and a few Cassandra nodes? Is that possible?

**Avi Kivity** 00:46:45 That's a common question but in fact it's not possible while the protocols are very similar they are not wire compatible and we decided not to go for wire compatibility for two reasons. One of them is that it would restrict our freedom of implementation and we wanted to have more freedom in the wire protocol and the other was that we figured that many users would not like to integrate scylla into a Cassandra cluster because then it might destabilize the Cassandra cluster and they might not want to do that while they are migrating. So the typical migration path is to have two parallel clusters running and the client would send each write into both clusters in order to make sure that the data is consistent and at some point in time when you are confident and everything is working as expected you switch over to scylla and continue with just one cluster.

**Nishant Suneja** 00:47:43 So this is the migration strategy if an enterprise wants to move from Cassandra to the ScyllaDB. 

**Avi Kivity** 00:47:50 Indeed and we've seen it done many times with zero migrations with zero downtime you have exactly the same data on both clusters and you have a period of time during which you can compare the data and make sure that everything works as expected before you make the decision.

**Nishant Suneja** 00:48:10 Great. ScyllaDB have a notion of transactions because I know that Cassandra claims to have the lightweight transaction which are a compare and replace operations. Does ScyllaDB have the same guarantees or does it provide more complicated transaction strategies?

**Avi Kivity** 00:48:35 All the lightweight transactions is one of the features which are still being implemented so it's not available yet we hope to have it available soon and in fact we do plan to expand it once it's available into multi-key transactions we know those are very useful and we also want to use transactions internally for things like cluster management so instead of having kind of eventual consistency model when you add or remove nodes do that as a transaction and also we would like to base our materialist views feature which already exists and it's shipping we would like to base that on transactions since it simplifies the implementation by quite a lot and it will also improve performance so we plan to have extensive use of transactions in ScyllaDB.

**Nishant Suneja** 00:49:27 Great. Can you briefly describe what a lightweight transaction is and how is ScyllaDB implementing them in the future release?

**Avi Kivity** 00:49:38 Yes so we plan to use the Raft protocol instead of the older Paxos protocol in order to establish a leader for every group of partitions and that leader will coordinate all of the writes that go into a particular group of partitions and this way it will be able to prevent concurrent writes to the same partitions and we order them in a way that is consistent across all the readers.

**Nishant Suneja** 00:50:16 I see. So moving on ScyllaDB claims to be the drop in replacement of Cassandra. This would imply an application level API compatibility and a complete support for the Cassandra query language. Is that always true or there are places where the two diverge?

**Avi Kivity** 00:50:36 So we try not to diverge sometimes we add extensions but we try to make sure that they don't conflict with the Cassandra language in any way and of course there are sometimes gaps when we haven't completed a feature and one example is the lightweight transactions. But these are always things that are just in progress and not some things that we're implementing in a different way on the client level. So it's not just the language that is the same it's also the protocol and so you can use a driver the same driver that you've always used and the application will work just the same and most of the times when we have an application migrating from Cassandra to Scylla it needs zero changes it just starts working with the higher throughput and lower latency.

**Nishant Suneja** 00:51:30 So like Cassandra ScyllaDB has a support of constructing multi data center rings to provide higher availability SLA guarantees. However, what features does ScyllaDB provide for live backup and restore in case of a region failure if for example the customer is hosting a ScyllaDB cluster on AWS?

**Avi Kivity** 00:51:52 Yes, so there are several features that increase availability. One of them is support for availability zones so Scylla can make sure that there is one replica for a given piece of data on each availability zone. So if it's just an availability zone that dies then you can recover the data from the two surviving availability zones. Another feature is multiple data center so if you have a region or a data center drop you can rebuild it from other data centers that have survived and of course for a complete disaster you have a backup and restore so ScyllaDB supports the snapshots you can snapshot the database at any point and you get the set of files that you can then copy to an offline storage location from which you can reconstruct the database as it was at the point of backup.

**Nishant Suneja** 00:52:46 So this is just for the clarity this is not the live backup this is more of reconstructing the data from enough and of and a touch storage which could potentially require some downtime for the application.Am I right?

**Avi Kivity** 00:53:06 Yes that's correct for for live backup you simply create another data center that is used for that is being replicated to and that other data center will contain all of the data that your database has and you can also switch your applications to that and in fact most deployments behave like that it's it's a multi master database so you can have recent writes to all of the data centers in parallel you can have your applications run only on one data center and switch over to another data center in the case of disaster or if you design your applications you can also have them running on all data centers all of the time and just increase the number of instances in case you have a reduction of capability in one of the data centers.

**Nishant Suneja** 00:53:54 So are these two data centers and that centers in continuous sync of each other are they synchronizing continuously in an asynchronous fashion or how how far behind one data center is as compared to the other data center when replicating writes.

**Avi Kivity** 00:54:16 So both modes are supported if you choose you can have synchronous replication so that when you issue a write to the database it will wait until the other data centers have been fully synchronized and that gives you a high guarantee of consistency but it trades off latency so you will have to wait for the round trip to those other data centers. The other option is have asynchronous replication in which case you get the response only from the local replicas and still will continue replication in the background typically it's just a few hundred milliseconds delay just the time takes for the data to propagate and Scylla also tries to make tries very hard to make sure that the data is replicated by writing the data into local disk and trying to replicate it later in case it fails if you have a network failure and also there are facilities like repair which allows you to re-synchronize in case you had a loss of connection between between two data centers but in general the database is fully synchronized at all time.

**Nishant Suneja** 00:55:28 So just for the clarity the scenario where you described about asynchronous replication and also asynchronous replication offering are you talking about replication across two different ScyllaDB clusters or are you talking about replication between a replica copy for a keyspace within a single ScyllaDB cluster?

**Avi Kivity** 00:55:54 It's a single cluster that is spread across multiple data centers. The database is data-center aware and so are the client drivers so you can ask the client drivers to only access the local data center in order to reduce latency and the other data centers are just being replicated too and since multi-master you can also have applications in other data centers accessing their local data centers the nodes in the local data centers and reading and writing the same data.

**Nishant Suneja** 00:56:27 Great. So one last question which I have are the upcoming features in ScyllaDB. Can you briefly tell us what are you working on and when can we expect them to be released?

**Avi Kivity** 00:56:39 Yes, so two features which we've worked on for a while and will be released in 3.0 that all are support for a new ScyllaDB format also compatible with Cassandra and that brings improved support for large partitions so queries within large partitions will be faster and also the amount of storage that is used will be lower so less storage overhead. The other feature which we've been working on for a long while is Materialized views so this has been available in experimental mode for a long while but in 3.0 it will be available for production and other features that we are working on lightweight transactions like I mentioned we also hope to bring tiered storage so that you could use different types of storage in the same node you will have expensive and fast storage as a fastier and less expensive and slower storage for your slow tier an example of that can be the NVMA disks that are available on Amazon instances and EBS disks which are slower but less expensive and by using a mix of fast and slow storage you can reduce your storage costs while still providing a high performance.

**Nishant Suneja** 00:58:00 Great. Do you plan to track the Cassandra features in the future or branch off and become something completely different?

**Avi Kivity** 00:58:10 In general we do plan to continue providing the Cassandra features. The Cassandra ecosystem is great there are lots of drivers for all languages and other ecosystem participants like Spark and Presto that work with Cassandra so Cassandra compatibility has been good for us. We will also do our own things we are not restricted to just doing Cassandra compatibility so we will bring out one features as well but in general we do plan to bring all of the good Cassandra features to ScyllaDB.

**Nishant Suneja** 00:58:45 Great. So are there any questions I should have asked or something you would like to say to our audience?

**Avi Kivity** 00:58:54 I think not I think that you've asked really good questions. I think that one thing I can say is that you can try ScyllaDB using the test drive system that we have which spins up a cluster for an hour and that allows you to see ScyllaDB in action. It's really great to go in there into a shell into a shell on one of the nodes and look at how all the cores are utilized and look at the disk utilization and throughput it's great to see the disk running at multiple gigabytes per second and all of the cores at 100% for me it's always a thrill to see every cycle being squeezed from those machines.

**Nishant Suneja** 00:59:39 Right. So how can people follow you?

**Avi Kivity** 00:59:42 Oh so you can follow me on Twitter I might be with the sarcastic sometimes but they also give some good information. You can follow the ScyllaDB blog where I write some blog posts and we have a great technical blog so which not only covers new features and things about how to use Scylla but also the decisions behind Scylla and more low level stuff so things like what we've been discussing in this podcast. So if what we talked about is interesting to you I really recommend following the ScyllaDB post the user and the technical blog.

**Nishant Suneja** 01:00:24 So Avi thank you for coming on the show.

**Avi Kivity** 01:00:26 Oh it was my pleasure.

**Nishant Suneja** 01:00:28 This is Nishant Sunasia for Software Engineering Radio thank you for listening.



Thanks for listening to SE Radio an educational program brought to you by IEEE Software magazine. For more about the podcast including other episodes visit our website at se-radio.net. To provide feedback you can comment on each episode on the website or reach us on LinkedIn, Facebook, Twitter or through our Slack channel at se-radio.slack.com. You can also email us at team at se-radio.net. This and all other episodes of SE Radio is licensed under Creative Commons license 2.5. Thanks for listening.